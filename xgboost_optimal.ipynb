{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7edfa05-ede7-4f82-add8-41eb5c91bc41",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "### Cleanup\n",
    "\n",
    "``` python\n",
    "import pandas as pd\n",
    "\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "\n",
    "url = 'https://github.com/mattharrison/datasets/raw/master/data/'\\\n",
    "    'kaggle-survey-2018.zip'\n",
    "fname = 'kaggle-survey-2018.zip'\n",
    "member_name = 'multipleChoiceResponses.csv'\n",
    "\n",
    "\n",
    "def extract_zip(src, dst, member_name):\n",
    "    \"\"\"Extract a member file from a zip file and read it into a pandas \n",
    "    DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        src (str): URL of the zip file to be downloaded and extracted.\n",
    "        dst (str): Local file path where the zip file will be written.\n",
    "        member_name (str): Name of the member file inside the zip file \n",
    "            to be read into a DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame containing the contents of the \n",
    "            member file.\n",
    "    \"\"\"    \n",
    "    url = src\n",
    "    fname = dst\n",
    "    fin = urllib.request.urlopen(url)\n",
    "    data = fin.read()\n",
    "    with open(dst, mode='wb') as fout:\n",
    "        fout.write(data)\n",
    "    with zipfile.ZipFile(dst) as z:\n",
    "        kag = pd.read_csv(z.open(member_name))\n",
    "        kag_questions = kag.iloc[0]\n",
    "        raw = kag.iloc[1:]\n",
    "        return raw\n",
    "\n",
    "raw = extract_zip(url, fname, member_name)        \n",
    "```\n",
    "\n",
    "### Cleanup Pipeline\n",
    "\n",
    "``` python\n",
    "def tweak_kag(df_: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Tweak the Kaggle survey data and return a new DataFrame.\n",
    "\n",
    "    This function takes a Pandas DataFrame containing Kaggle \n",
    "    survey data as input and returns a new DataFrame. The \n",
    "    modifications include extracting and transforming certain \n",
    "    columns, renaming columns, and selecting a subset of columns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_ : pd.DataFrame\n",
    "        The input DataFrame containing Kaggle survey data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The new DataFrame with the modified and selected columns.\n",
    "    \"\"\"    \n",
    "    return (df_\n",
    "            .assign(age=df_.Q2.str.slice(0,2).astype(int),\n",
    "                    education=df_.Q4.replace({'Master’s degree': 18,\n",
    "                         'Bachelor’s degree': 16,\n",
    "                         'Doctoral degree': 20,\n",
    "'Some college/university study without earning a bachelor’s degree': 13,\n",
    "                         'Professional degree': 19,\n",
    "                         'I prefer not to answer': None,\n",
    "                         'No formal education past high school': 12}),\n",
    "                    major=(df_.Q5\n",
    "                              .pipe(topn, n=3)\n",
    "                              .replace({\n",
    "                        'Computer science (software engineering, etc.)': 'cs',\n",
    "                        'Engineering (non-computer focused)': 'eng',\n",
    "                        'Mathematics or statistics': 'stat'})\n",
    "                         ),\n",
    "                    years_exp=(df_.Q8.str.replace('+','', regex=False)\n",
    "                           .str.split('-', expand=True)\n",
    "                           .iloc[:,0]\n",
    "                           .astype(float)),\n",
    "                    compensation=(df_.Q9.str.replace('+','', regex=False)\n",
    "                           .str.replace(',','', regex=False)\n",
    "                           .str.replace('500000', '500', regex=False)\n",
    "  .str.replace('I do not wish to disclose my approximate yearly compensation',\n",
    "             '0', regex=False)\n",
    "                           .str.split('-', expand=True)\n",
    "                           .iloc[:,0]\n",
    "                           .fillna(0)\n",
    "                           .astype(int)\n",
    "                           .mul(1_000)\n",
    "                                    ),\n",
    "                    python=df_.Q16_Part_1.fillna(0).replace('Python', 1),\n",
    "                    r=df_.Q16_Part_2.fillna(0).replace('R', 1),\n",
    "                    sql=df_.Q16_Part_3.fillna(0).replace('SQL', 1)\n",
    "               )#assign\n",
    "        .rename(columns=lambda col:col.replace(' ', '_'))\n",
    "        .loc[:, 'Q1,Q3,age,education,major,years_exp,compensation,'\n",
    "                'python,r,sql'.split(',')]   \n",
    "       )\n",
    "\n",
    "        \n",
    "def topn(ser, n=5, default='other'):\n",
    "    \"\"\"\n",
    "    Replace all values in a Pandas Series that are not among \n",
    "    the top `n` most frequent values with a default value.\n",
    "\n",
    "    This function takes a Pandas Series and returns a new \n",
    "    Series with the values replaced as described above. The \n",
    "    top `n` most frequent values are determined using the \n",
    "    `value_counts` method of the input Series.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ser : pd.Series\n",
    "        The input Series.\n",
    "    n : int, optional\n",
    "        The number of most frequent values to keep. The \n",
    "        default value is 5.\n",
    "    default : str, optional\n",
    "        The default value to use for values that are not among \n",
    "        the top `n` most frequent values. The default value is \n",
    "        'other'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        The modified Series with the values replaced.\n",
    "    \"\"\"    \n",
    "    counts = ser.value_counts()\n",
    "    return ser.where(ser.isin(counts.index[:n]), default)\n",
    "```\n",
    "\n",
    "``` python\n",
    "from feature_engine import encoding, imputation\n",
    "from sklearn import base, pipeline\n",
    "\n",
    "\n",
    "class TweakKagTransformer(base.BaseEstimator,\n",
    "    base.TransformerMixin):\n",
    "    \"\"\"\n",
    "    A transformer for tweaking Kaggle survey data.\n",
    "\n",
    "    This transformer takes a Pandas DataFrame containing \n",
    "    Kaggle survey data as input and returns a new version of \n",
    "    the DataFrame. The modifications include extracting and \n",
    "    transforming certain columns, renaming columns, and \n",
    "    selecting a subset of columns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ycol : str, optional\n",
    "        The name of the column to be used as the target variable. \n",
    "        If not specified, the target variable will not be set.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    ycol : str\n",
    "        The name of the column to be used as the target variable.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, ycol=None):\n",
    "        self.ycol = ycol\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return tweak_kag(X)\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "```\n",
    "\n",
    "``` python\n",
    "def get_rawX_y(df, y_col):\n",
    "    raw = (df\n",
    "            .query('Q3.isin([\"United States of America\", \"China\", \"India\"]) '\n",
    "               'and Q6.isin([\"Data Scientist\", \"Software Engineer\"])')\n",
    "          )\n",
    "    return raw.drop(columns=[y_col]), raw[y_col]\n",
    "\n",
    "\n",
    "## Create a pipeline\n",
    "kag_pl = pipeline.Pipeline(\n",
    "    [('tweak', TweakKagTransformer()),\n",
    "     ('cat', encoding.OneHotEncoder(top_categories=5, drop_last=True, \n",
    "           variables=['Q1', 'Q3', 'major'])),\n",
    "     ('num_impute', imputation.MeanMedianImputer(imputation_method='median',\n",
    "          variables=['education', 'years_exp']))]\n",
    ")\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> from sklearn import model_selection\n",
    ">>> kag_X, kag_y = get_rawX_y(raw, 'Q6')\n",
    "    \n",
    ">>> kag_X_train, kag_X_test, kag_y_train, kag_y_test = \\\n",
    "...    model_selection.train_test_split(\n",
    "...        kag_X, kag_y, test_size=.3, random_state=42, stratify=kag_y)\n",
    "\n",
    ">>> X_train = kag_pl.fit_transform(kag_X_train, kag_y_train)\n",
    ">>> X_test = kag_pl.transform(kag_X_test)\n",
    ">>> print(X_train)\n",
    "       age  education  years_exp  ...  major_other  major_eng  major_stat\n",
    "587     25       18.0        4.0  ...            1          0           0\n",
    "3065    22       16.0        1.0  ...            0          0           0\n",
    "8435    22       18.0        1.0  ...            1          0           0\n",
    "3110    40       20.0        3.0  ...            1          0           0\n",
    "16372   45       12.0        5.0  ...            1          0           0\n",
    "...    ...        ...        ...  ...          ...        ...         ...\n",
    "16608   25       16.0        2.0  ...            0          0           0\n",
    "7325    18       16.0        1.0  ...            0          0           0\n",
    "21810   18       16.0        2.0  ...            0          0           0\n",
    "4917    25       18.0        1.0  ...            0          0           1\n",
    "639     25       18.0        1.0  ...            0          0           0\n",
    "\n",
    "[2110 rows x 18 columns]\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> kag_y_train\n",
    "587      Software Engineer\n",
    "3065        Data Scientist\n",
    "8435        Data Scientist\n",
    "3110        Data Scientist\n",
    "16372    Software Engineer\n",
    "               ...        \n",
    "16608    Software Engineer\n",
    "7325     Software Engineer\n",
    "21810       Data Scientist\n",
    "4917        Data Scientist\n",
    "639         Data Scientist\n",
    "Name: Q6, Length: 2110, dtype: object\n",
    "```\n",
    "\n",
    "## Exploratory Data Analysis\n",
    "\n",
    "### Correlations\n",
    "\n",
    "``` python\n",
    "(X_train\n",
    " .assign(data_scientist = kag_y_train == 'Data Scientist')\n",
    " .corr(method='spearman')\n",
    " .style\n",
    " .background_gradient(cmap='RdBu', vmax=1, vmin=-1)\n",
    " .set_sticky(axis='index')\n",
    ")\n",
    "```\n",
    "\n",
    "### Bar Plot\n",
    "\n",
    "``` python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "(X_train\n",
    " .assign(data_scientist = kag_y_train)\n",
    " .groupby('r')\n",
    " .data_scientist\n",
    " .value_counts()\n",
    " .unstack()\n",
    " .plot.bar(ax=ax)\n",
    ")\n",
    "```\n",
    "\n",
    "``` python\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "(pd.crosstab(index=X_train['major_cs'], \n",
    "             columns=kag_y)\n",
    "    .plot.bar(ax=ax)\n",
    ")\n",
    "```\n",
    "\n",
    "``` python\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "(X_train\n",
    " .plot.scatter(x='years_exp', y='compensation', alpha=.3, ax=ax, c='purple')\n",
    ")\n",
    "```\n",
    "\n",
    "``` python\n",
    "import seaborn.objects as so\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "(so\n",
    " .Plot(X_train.assign(title=kag_y_train), x='years_exp', y='compensation', color='title')\n",
    " .add(so.Dots(alpha=.3, pointsize=2), so.Jitter(x=.5, y=10_000))\n",
    " .add(so.Line(), so.PolyFit())\n",
    " .on(fig)  # not required unless saving to image\n",
    " .plot()   # ditto\n",
    ")\n",
    "```\n",
    "\n",
    "``` python\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "(so\n",
    " .Plot(X_train\n",
    "       #.query('compensation < 200_000 and years_exp < 16')\n",
    "       .assign(\n",
    "         title=kag_y_train,\n",
    "         country=(X_train\n",
    "             .loc[:, 'Q3_United States of America': 'Q3_China']\n",
    "             .idxmax(axis='columns')\n",
    "            )\n",
    "       ), x='years_exp', y='compensation', color='title')\n",
    " .facet('country')\n",
    " .add(so.Dots(alpha=.01, pointsize=2, color='grey' ), so.Jitter(x=.5, y=10_000), col=None)\n",
    " .add(so.Dots(alpha=.5, pointsize=1.5), so.Jitter(x=.5, y=10_000))\n",
    " .add(so.Line(pointsize=1), so.PolyFit(order=2))\n",
    " .scale(x=so.Continuous().tick(at=[0,1,2,3,4,5]))\n",
    " .limit(y=(-10_000, 200_000), x=(-1, 6))  # zoom in with this not .query (above)\n",
    " .on(fig)  # not required unless saving to image\n",
    " .plot()   # ditto\n",
    ")\n",
    "```\n",
    "\n",
    "## Tree Creation\n",
    "\n",
    "### The Gini Coefficient\n",
    "\n",
    "``` python\n",
    "import numpy as np\n",
    "import numpy.random as rn\n",
    "\n",
    "pos_center = 12\n",
    "pos_count = 100\n",
    "neg_center = 7\n",
    "neg_count = 1000\n",
    "rs = rn.RandomState(rn.MT19937(rn.SeedSequence(42)))\n",
    "gini = pd.DataFrame({'value':\n",
    "    np.append((pos_center) + rs.randn(pos_count),\n",
    "              (neg_center) + rs.randn(neg_count)),\n",
    "                     'label':\n",
    "    ['pos']* pos_count + ['neg'] * neg_count})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "_ = (gini\n",
    " .groupby('label')\n",
    " [['value']]\n",
    "     .plot.hist(bins=30, alpha=.5, ax=ax, edgecolor='black')\n",
    ")\n",
    "ax.legend(['Negative', 'Positive'])\n",
    "```\n",
    "\n",
    "``` python\n",
    "def calc_gini(df, val_col, label_col, pos_val, split_point,\n",
    "              debug=False):\n",
    "    \"\"\"\n",
    "    This function calculates the Gini impurity of a dataset. Gini impurity \n",
    "    is a measure of the probability of a random sample being classified \n",
    "    incorrectly when a feature is used to split the data. The lower the \n",
    "    impurity, the better the split.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The dataframe containing the data\n",
    "    val_col (str): The column name of the feature used to split the data\n",
    "    label_col (str): The column name of the target variable\n",
    "    pos_val (str or int): The value of the target variable that represents \n",
    "        the positive class\n",
    "    split_point (float): The threshold used to split the data.\n",
    "    debug (bool): optional, when set to True, prints the calculated Gini\n",
    "        impurities and the final weighted average\n",
    "\n",
    "    Returns:\n",
    "    float: The weighted average of Gini impurity for the positive and \n",
    "        negative subsets.\n",
    "    \"\"\"    \n",
    "    ge_split = df[val_col] >= split_point\n",
    "    eq_pos = df[label_col] == pos_val\n",
    "    tp = df[ge_split & eq_pos].shape[0]\n",
    "    fp = df[ge_split & ~eq_pos].shape[0]\n",
    "    tn = df[~ge_split & ~eq_pos].shape[0]\n",
    "    fn = df[~ge_split & eq_pos].shape[0]\n",
    "    pos_size = tp+fp\n",
    "    neg_size = tn+fn\n",
    "    total_size = len(df)\n",
    "    if pos_size == 0:\n",
    "        gini_pos = 0\n",
    "    else:\n",
    "        gini_pos = 1 - (tp/pos_size)**2 - (fp/pos_size)**2\n",
    "    if neg_size == 0:\n",
    "        gini_neg = 0\n",
    "    else:\n",
    "        gini_neg = 1 - (tn/neg_size)**2 - (fn/neg_size)**2\n",
    "    weighted_avg = gini_pos * (pos_size/total_size) + \\\n",
    "                   gini_neg * (neg_size/total_size)\n",
    "    if debug:\n",
    "        print(f'{gini_pos=:.3} {gini_neg=:.3} {weighted_avg=:.3}')\n",
    "    return weighted_avg\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> calc_gini(gini, val_col='value', label_col='label', pos_val='pos',\n",
    "...          split_point=9.24, debug=True)\n",
    "gini_pos=0.217 gini_neg=0.00202 weighted_avg=0.0241\n",
    "0.024117224644432264\n",
    "```\n",
    "\n",
    "``` python\n",
    "values = np.arange(5, 15, .1)\n",
    "ginis = []\n",
    "for v in values:\n",
    "    ginis.append(calc_gini(gini, val_col='value', label_col='label',\n",
    "                           pos_val='pos', split_point=v))\n",
    "fig, ax = plt.subplots(figsize=(8, 4))    \n",
    "ax.plot(values, ginis)\n",
    "ax.set_title('Gini Coefficient')\n",
    "ax.set_ylabel('Gini Coefficient')\n",
    "ax.set_xlabel('Split Point')\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> pd.Series(ginis, index=values).loc[9.5:10.5]\n",
    "9.6     0.013703\n",
    "9.7     0.010470\n",
    "9.8     0.007193\n",
    "9.9     0.005429\n",
    "10.0    0.007238\n",
    "10.1    0.005438\n",
    "10.2    0.005438\n",
    "10.3    0.007244\n",
    "10.4    0.009046\n",
    "10.5    0.009046\n",
    "dtype: float64\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> print(pd.DataFrame({'gini':ginis, 'split':values})\n",
    "...  .query('gini <= gini.min()')\n",
    "... )\n",
    "        gini  split\n",
    "49  0.005429    9.9\n",
    "```\n",
    "\n",
    "### Coefficients in Trees\n",
    "\n",
    "``` python\n",
    "from sklearn import tree\n",
    "stump = tree.DecisionTreeClassifier(max_depth=1)\n",
    "stump.fit(gini[['value']], gini.label)\n",
    "```\n",
    "\n",
    "``` python\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "tree.plot_tree(stump, feature_names=['value'],\n",
    "               filled=True, \n",
    "               class_names=stump.classes_,\n",
    "               ax=ax)\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> gini_pos = 0.039\n",
    ">>> gini_neg = 0.002\n",
    ">>> pos_size = 101\n",
    ">>> neg_size = 999\n",
    ">>> total_size = pos_size + neg_size\n",
    ">>> weighted_avg = gini_pos * (pos_size/total_size) + \\\n",
    "...               gini_neg * (neg_size/total_size)\n",
    ">>> print(weighted_avg)\n",
    "0.005397272727272727\n",
    "```\n",
    "\n",
    "``` python\n",
    "import xgboost as xgb\n",
    "xg_stump = xgb.XGBClassifier(n_estimators=1, max_depth=1)                 \n",
    "xg_stump.fit(gini[['value']], (gini.label== 'pos'))\n",
    "```\n",
    "\n",
    "``` python\n",
    "xgb.plot_tree(xg_stump, num_trees=0)\n",
    "```\n",
    "\n",
    "``` python\n",
    "import subprocess\n",
    "def my_dot_export(xg, num_trees, filename, title='', direction='TB'):\n",
    "    \"\"\"Exports a specified number of trees from an XGBoost model as a graph \n",
    "    visualization in dot and png formats.\n",
    "\n",
    "    Args:\n",
    "        xg: An XGBoost model.\n",
    "        num_trees: The number of tree to export.\n",
    "        filename: The name of the file to save the exported visualization.\n",
    "        title: The title to display on the graph visualization (optional).\n",
    "        direction: The direction to lay out the graph, either 'TB' (top to \n",
    "            bottom) or 'LR' (left to right) (optional).\n",
    "    \"\"\"\n",
    "    res = xgb.to_graphviz(xg, num_trees=num_trees)\n",
    "    content = f'''    node [fontname = \"Roboto Condensed\"];\n",
    "    edge [fontname = \"Roboto Thin\"];\n",
    "    label = \"{title}\"\n",
    "    fontname = \"Roboto Condensed\"\n",
    "    '''\n",
    "    out = res.source.replace('graph [ rankdir=TB ]', \n",
    "                             f'graph [ rankdir={direction} ];\\n {content}')\n",
    "    # dot -Gdpi=300 -Tpng -ocourseflow.png courseflow.dot \n",
    "    dot_filename = filename\n",
    "    with open(dot_filename, 'w') as fout:\n",
    "        fout.write(out)\n",
    "    png_filename = dot_filename.replace('.dot', '.png')\n",
    "    subprocess.run(f'dot -Gdpi=300 -Tpng -o{png_filename} {dot_filename}'.split())\n",
    "```\n",
    "\n",
    "``` python\n",
    "my_dot_export(xg_stump, num_trees=0, filename='img/stump_xg.dot', title='A demo stump')    \n",
    "```\n",
    "\n",
    "### Another Visualization Tool\n",
    "\n",
    "``` python\n",
    "import dtreeviz\n",
    "viz = dtreeviz.model(xg_stump, X_train=gini[['value']], \n",
    "                     y_train=gini.label=='pos',\n",
    "    target_name='positive',\n",
    "    feature_names=['value'], class_names=['negative', 'positive'],\n",
    "    tree_index=0)\n",
    "viz.view()\n",
    "```\n",
    "\n",
    "## Stumps on Real Data\n",
    "\n",
    "### Scikit-learn stump on real data\n",
    "\n",
    "``` python\n",
    "stump_dt = tree.DecisionTreeClassifier(max_depth=1)\n",
    "X_train = kag_pl.fit_transform(kag_X_train)\n",
    "stump_dt.fit(X_train, kag_y_train)\n",
    "```\n",
    "\n",
    "``` python\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "features = list(c for c in X_train.columns)\n",
    "tree.plot_tree(stump_dt, feature_names=features, \n",
    "               filled=True, \n",
    "               class_names=stump_dt.classes_,\n",
    "               ax=ax)\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> X_test = kag_pl.transform(kag_X_test)\n",
    ">>> stump_dt.score(X_test, kag_y_test)\n",
    "0.6243093922651933\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> from sklearn import dummy\n",
    ">>> dummy_model = dummy.DummyClassifier()\n",
    ">>> dummy_model.fit(X_train, kag_y_train)\n",
    ">>> dummy_model.score(X_test, kag_y_test)\n",
    "0.5458563535911602\n",
    "```\n",
    "\n",
    "### Decision Stump with XGBoost\n",
    "\n",
    "``` python\n",
    "import xgboost as xgb\n",
    "kag_stump = xgb.XGBClassifier(n_estimators=1, max_depth=1)\n",
    "kag_stump.fit(X_train, kag_y_train)\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> print(kag_y_train)\n",
    "587      Software Engineer\n",
    "3065        Data Scientist\n",
    "8435        Data Scientist\n",
    "3110        Data Scientist\n",
    "16372    Software Engineer\n",
    "               ...        \n",
    "16608    Software Engineer\n",
    "7325     Software Engineer\n",
    "21810       Data Scientist\n",
    "4917        Data Scientist\n",
    "639         Data Scientist\n",
    "Name: Q6, Length: 2110, dtype: object\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> print(kag_y_train == 'Software Engineer')\n",
    "587       True\n",
    "3065     False\n",
    "8435     False\n",
    "3110     False\n",
    "16372     True\n",
    "         ...  \n",
    "16608     True\n",
    "7325      True\n",
    "21810    False\n",
    "4917     False\n",
    "639      False\n",
    "Name: Q6, Length: 2110, dtype: bool\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> from sklearn import preprocessing\n",
    ">>> label_encoder = preprocessing.LabelEncoder()\n",
    ">>> y_train = label_encoder.fit_transform(kag_y_train)\n",
    ">>> y_test = label_encoder.transform(kag_y_test)\n",
    ">>> y_test[:5]\n",
    "array([1, 0, 0, 1, 1])\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> label_encoder.classes_\n",
    "array(['Data Scientist', 'Software Engineer'], dtype=object)\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> label_encoder.inverse_transform([0, 1])\n",
    "array(['Data Scientist', 'Software Engineer'], dtype=object)\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> kag_stump = xgb.XGBClassifier(n_estimators=1, max_depth=1)\n",
    ">>> kag_stump.fit(X_train, y_train)\n",
    ">>> kag_stump.score(X_test, y_test)\n",
    "0.6243093922651933\n",
    "```\n",
    "\n",
    "``` python\n",
    "my_dot_export(kag_stump, num_trees=0, filename='img/stump_xg_kag.dot', \n",
    "              title='XGBoost Stump')    \n",
    "```\n",
    "\n",
    "### Values in the XGBoost Tree\n",
    "\n",
    "``` pycon\n",
    ">>> kag_stump.classes_\n",
    "array([0, 1])\n",
    "```\n",
    "\n",
    "``` python\n",
    "import numpy as np\n",
    "def inv_logit(p: float) -> float:\n",
    "    \"\"\"\n",
    "    Compute the inverse logit function of a given value.\n",
    "\n",
    "    The inverse logit function is defined as:\n",
    "        f(p) = exp(p) / (1 + exp(p))\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    p : float\n",
    "        The input value to the inverse logit function.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The output of the inverse logit function.\n",
    "    \"\"\"\n",
    "    return np.exp(p) / (1 + np.exp(p))\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> inv_logit(.0717741922)\n",
    "0.5179358489487103\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> inv_logit(-.3592)\n",
    "0.41115323716754393\n",
    "```\n",
    "\n",
    "``` python\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "vals = np.linspace(-7, 7)\n",
    "ax.plot(vals, inv_logit(vals))\n",
    "ax.annotate('Crossover point', (0,.5), (-5,.8), arrowprops={'color':'k'}) \n",
    "ax.annotate('Predict Positive', (5,.6), (1,.6), va='center', arrowprops={'color':'k'}) \n",
    "ax.annotate('Predict Negative', (-5,.4), (-3,.4), va='center', arrowprops={'color':'k'}) \n",
    "```\n",
    "\n",
    "### Summary\n",
    "\n",
    "### Exercises\n",
    "\n",
    "## Model Complexity & Hyperparameters\n",
    "\n",
    "### Underfit\n",
    "\n",
    "``` pycon\n",
    ">>> underfit = tree.DecisionTreeClassifier(max_depth=1)\n",
    ">>> X_train = kag_pl.fit_transform(kag_X_train)\n",
    ">>> underfit.fit(X_train, kag_y_train)\n",
    ">>> underfit.score(X_test, kag_y_test)\n",
    "0.6243093922651933\n",
    "```\n",
    "\n",
    "### Growing a Tree\n",
    "\n",
    "### Overfitting\n",
    "\n",
    "### Overfitting with Decision Trees\n",
    "\n",
    "``` pycon\n",
    ">>> hi_variance = tree.DecisionTreeClassifier(max_depth=None)\n",
    ">>> X_train = kag_pl.fit_transform(kag_X_train)\n",
    ">>> hi_variance.fit(X_train, kag_y_train)\n",
    ">>> hi_variance.score(X_test, kag_y_test)\n",
    "0.6629834254143646\n",
    "```\n",
    "\n",
    "``` python\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "features = list(c for c in X_train.columns)\n",
    "tree.plot_tree(hi_variance, feature_names=features, filled=True)\n",
    "```\n",
    "\n",
    "``` python\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "features = list(c for c in X_train.columns)\n",
    "tree.plot_tree(hi_variance, feature_names=features, filled=True, \n",
    "                  class_names=hi_variance.classes_,\n",
    "                max_depth=2, fontsize=6)\n",
    "```\n",
    "\n",
    "### Summary\n",
    "\n",
    "### Exercises\n",
    "\n",
    "## Tree Hyperparameters\n",
    "\n",
    "### Decision Tree Hyperparameters\n",
    "\n",
    "``` pycon\n",
    ">>> stump.get_params()\n",
    "{'ccp_alpha': 0.0,\n",
    " 'class_weight': None,\n",
    " 'criterion': 'gini',\n",
    " 'max_depth': 1,\n",
    " 'max_features': None,\n",
    " 'max_leaf_nodes': None,\n",
    " 'min_impurity_decrease': 0.0,\n",
    " 'min_samples_leaf': 1,\n",
    " 'min_samples_split': 2,\n",
    " 'min_weight_fraction_leaf': 0.0,\n",
    " 'random_state': None,\n",
    " 'splitter': 'best'}\n",
    "```\n",
    "\n",
    "### Tracking changes with Validation Curves\n",
    "\n",
    "``` python\n",
    "accuracies = []\n",
    "for depth in range(1, 15):\n",
    "    between = tree.DecisionTreeClassifier(max_depth=depth)\n",
    "    between.fit(X_train, kag_y_train)\n",
    "    accuracies.append(between.score(X_test, kag_y_test))\n",
    "fig, ax = plt.subplots(figsize=(10,4))    \n",
    "(pd.Series(accuracies, name='Accuracy', index=range(1, len(accuracies)+1))\n",
    " .plot(ax=ax, title='Accuracy at a given Tree Depth'))\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_xlabel('max_depth')\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> between = tree.DecisionTreeClassifier(max_depth=7)\n",
    ">>> between.fit(X_train, kag_y_train)\n",
    ">>> between.score(X_test, kag_y_test)\n",
    "0.7359116022099448\n",
    "```\n",
    "\n",
    "### Leveraging Yellowbrick\n",
    "\n",
    "``` python\n",
    "from yellowbrick.model_selection import validation_curve\n",
    "fig, ax = plt.subplots(figsize=(10,4))    \n",
    "viz = validation_curve(tree.DecisionTreeClassifier(),\n",
    "    X=pd.concat([X_train, X_test]),\n",
    "    y=pd.concat([kag_y_train, kag_y_test]),   \n",
    "    param_name='max_depth', param_range=range(1,14),\n",
    "    scoring='accuracy', cv=5, ax=ax, n_jobs=6)                           \n",
    "```\n",
    "\n",
    "### Grid Search\n",
    "\n",
    "``` python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "params = {\n",
    "    'max_depth': [3, 5, 7, 8],\n",
    "    'min_samples_leaf': [1, 3, 4, 5, 6],\n",
    "    'min_samples_split': [2, 3, 4, 5, 6],\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=tree.DecisionTreeClassifier(), \n",
    "                           param_grid=params, cv=4, n_jobs=-1, \n",
    "                           verbose=1, scoring=\"accuracy\")\n",
    "grid_search.fit(pd.concat([X_train, X_test]),\n",
    "    pd.concat([kag_y_train, kag_y_test]))\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> grid_search.best_params_\n",
    "{'max_depth': 7, 'min_samples_leaf': 5, 'min_samples_split': 6}\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> between2 = tree.DecisionTreeClassifier(**grid_search.best_params_)\n",
    ">>> between2.fit(X_train, kag_y_train)\n",
    ">>> between2.score(X_test, kag_y_test)\n",
    "0.7259668508287292\n",
    "```\n",
    "\n",
    "``` python\n",
    "# why is the score different than between_tree?\n",
    "(pd.DataFrame(grid_search.cv_results_)\n",
    " .sort_values(by='rank_test_score')\n",
    " .style\n",
    " .background_gradient(axis='rows')\n",
    ")\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> results = model_selection.cross_val_score(\n",
    "...    tree.DecisionTreeClassifier(max_depth=7),\n",
    "...    X=pd.concat([X_train, X_test], axis='index'),\n",
    "...    y=pd.concat([kag_y_train, kag_y_test], axis='index'),\n",
    "...    cv=4\n",
    "... )\n",
    "\n",
    ">>> results\n",
    "array([0.69628647, 0.73607427, 0.70291777, 0.7184595 ])\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> results.mean()\n",
    "0.7134345024851962\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> results = model_selection.cross_val_score(\n",
    "...    tree.DecisionTreeClassifier(max_depth=7, min_samples_leaf=5,\n",
    "...                                min_samples_split=2),\n",
    "...    X=pd.concat([X_train, X_test], axis='index'),\n",
    "...    y=pd.concat([kag_y_train, kag_y_test], axis='index'),\n",
    "...    cv=4\n",
    "... )\n",
    "\n",
    ">>> results\n",
    "array([0.70822281, 0.73740053, 0.70689655, 0.71580345])\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> results.mean()\n",
    "0.7170808366886126\n",
    "```\n",
    "\n",
    "### Summary\n",
    "\n",
    "### Exercises\n",
    "\n",
    "## Random Forest\n",
    "\n",
    "### Ensembles with Bagging\n",
    "\n",
    "### Scikit-learn Random Forest\n",
    "\n",
    "``` pycon\n",
    ">>> from sklearn import ensemble\n",
    ">>> rf = ensemble.RandomForestClassifier(random_state=42)\n",
    ">>> rf.fit(X_train, kag_y_train)\n",
    ">>> rf.score(X_test, kag_y_test)\n",
    "0.7237569060773481\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> rf.get_params()\n",
    "{'bootstrap': True,\n",
    " 'ccp_alpha': 0.0,\n",
    " 'class_weight': None,\n",
    " 'criterion': 'gini',\n",
    " 'max_depth': None,\n",
    " 'max_features': 'sqrt',\n",
    " 'max_leaf_nodes': None,\n",
    " 'max_samples': None,\n",
    " 'min_impurity_decrease': 0.0,\n",
    " 'min_samples_leaf': 1,\n",
    " 'min_samples_split': 2,\n",
    " 'min_weight_fraction_leaf': 0.0,\n",
    " 'n_estimators': 100,\n",
    " 'n_jobs': None,\n",
    " 'oob_score': False,\n",
    " 'random_state': 42,\n",
    " 'verbose': 0,\n",
    " 'warm_start': False}\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> len(rf.estimators_)\n",
    "100\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> print(rf.estimators_[0])\n",
    "DecisionTreeClassifier(max_features='sqrt', random_state=1608637542)\n",
    "```\n",
    "\n",
    "``` python\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "features = list(c for c in X_train.columns)\n",
    "tree.plot_tree(rf.estimators_[0], feature_names=features, \n",
    "               filled=True, class_names=rf.classes_, ax=ax,\n",
    "               max_depth=2, fontsize=6)\n",
    "```\n",
    "\n",
    "### XGBoost Random Forest\n",
    "\n",
    "``` pycon\n",
    ">>> import xgboost as xgb\n",
    ">>> rf_xg = xgb.XGBRFClassifier(random_state=42)\n",
    ">>> rf_xg.fit(X_train, y_train) \n",
    ">>> rf_xg.score(X_test, y_test)\n",
    "0.7447513812154696\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> rf_xg.get_params()\n",
    "{'colsample_bynode': 0.8,\n",
    " 'learning_rate': 1.0,\n",
    " 'reg_lambda': 1e-05,\n",
    " 'subsample': 0.8,\n",
    " 'objective': 'binary:logistic',\n",
    " 'use_label_encoder': None,\n",
    " 'base_score': 0.5,\n",
    " 'booster': 'gbtree',\n",
    " 'callbacks': None,\n",
    " 'colsample_bylevel': 1,\n",
    " 'colsample_bytree': 1,\n",
    " 'early_stopping_rounds': None,\n",
    " 'enable_categorical': False,\n",
    " 'eval_metric': None,\n",
    " 'feature_types': None,\n",
    " 'gamma': 0,\n",
    " 'gpu_id': -1,\n",
    " 'grow_policy': 'depthwise',\n",
    " 'importance_type': None,\n",
    " 'interaction_constraints': '',\n",
    " 'max_bin': 256,\n",
    " 'max_cat_threshold': 64,\n",
    " 'max_cat_to_onehot': 4,\n",
    " 'max_delta_step': 0,\n",
    " 'max_depth': 6,\n",
    " 'max_leaves': 0,\n",
    " 'min_child_weight': 1,\n",
    " 'missing': nan,\n",
    " 'monotone_constraints': '()',\n",
    " 'n_estimators': 100,\n",
    " 'n_jobs': 0,\n",
    " 'num_parallel_tree': 100,\n",
    " 'predictor': 'auto',\n",
    " 'random_state': 42,\n",
    " 'reg_alpha': 0,\n",
    " 'sampling_method': 'uniform',\n",
    " 'scale_pos_weight': 1,\n",
    " 'tree_method': 'exact',\n",
    " 'validate_parameters': 1,\n",
    " 'verbosity': None}\n",
    "```\n",
    "\n",
    "``` python\n",
    "fig, ax = plt.subplots(figsize=(6,12), dpi=600)\n",
    "xgb.plot_tree(rf_xg, num_trees=0, ax=ax, size='1,1')\n",
    "```\n",
    "\n",
    "``` python\n",
    "my_dot_export(rf_xg, num_trees=0, filename='img/rf_xg_kag.dot', \n",
    "              title='First Random Forest Tree', direction='LR')    \n",
    "```\n",
    "\n",
    "``` python\n",
    "viz = dtreeviz.model(rf_xg, X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    target_name='Job', feature_names=list(X_train.columns), \n",
    "    class_names=['DS', 'SE'], tree_index=0)\n",
    "viz.view(depth_range_to_display=[0,2])\n",
    "```\n",
    "\n",
    "### Random Forest Hyperparameters\n",
    "\n",
    "### Training the Number of Trees in the Forest\n",
    "\n",
    "``` python\n",
    "from yellowbrick.model_selection import validation_curve\n",
    "fig, ax = plt.subplots(figsize=(10,4))    \n",
    "viz = validation_curve(xgb.XGBClassifier(random_state=42),\n",
    "    x=pd.concat([X_train, X_test], axis='index'),\n",
    "    y=np.concatenate([y_train, y_test]),\n",
    "    param_name='n_estimators', param_range=range(1, 100, 2),\n",
    "    scoring='accuracy', cv=3, \n",
    "    ax=ax)                           \n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> rf_xg29 = xgb.XGBRFClassifier(random_state=42, n_estimators=29)\n",
    ">>> rf_xg29.fit(X_train, y_train) \n",
    ">>> rf_xg29.score(X_test, y_test)\n",
    "0.7480662983425415\n",
    "```\n",
    "\n",
    "### Summary\n",
    "\n",
    "### Exercises\n",
    "\n",
    "## XGBoost\n",
    "\n",
    "### Jargon\n",
    "\n",
    "### Benefits of Boosting\n",
    "\n",
    "### A Big Downside\n",
    "\n",
    "### Creating an XGBoost Model\n",
    "\n",
    "``` python\n",
    "%matplotlib inline\n",
    "\n",
    "import dtreeviz\n",
    "from feature_engine import encoding, imputation\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import base, compose, datasets, ensemble, \\\n",
    "    metrics, model_selection, pipeline, preprocessing, tree\n",
    "import scikitplot\n",
    "import xgboost as xgb\n",
    "import yellowbrick.model_selection as ms\n",
    "from yellowbrick import classifier\n",
    "\n",
    "import urllib\n",
    "import zipfile\n",
    "\n",
    "import xg_helpers as xhelp\n",
    "```\n",
    "\n",
    "``` python\n",
    "url = 'https://github.com/mattharrison/datasets/raw/master/data/'\\\n",
    "    'kaggle-survey-2018.zip'\n",
    "fname = 'kaggle-survey-2018.zip'\n",
    "member_name = 'multipleChoiceResponses.csv'\n",
    "\n",
    "raw = xhelp.extract_zip(url, fname, member_name)\n",
    "```\n",
    "\n",
    "``` python\n",
    "## Create raw X and raw y\n",
    "kag_X, kag_y = xhelp.get_rawX_y(raw, 'Q6')\n",
    "    \n",
    "## Split data    \n",
    "kag_X_train, kag_X_test, kag_y_train, kag_y_test = \\\n",
    "    model_selection.train_test_split(\n",
    "        kag_X, kag_y, test_size=.3, random_state=42, stratify=kag_y)    \n",
    "\n",
    "## Transform X with pipeline\n",
    "X_train = xhelp.kag_pl.fit_transform(kag_X_train)\n",
    "X_test = xhelp.kag_pl.transform(kag_X_test)\n",
    "\n",
    "## Transform y with label encoder\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "label_encoder.fit(kag_y_train)\n",
    "y_train = label_encoder.transform(kag_y_train)\n",
    "y_test = label_encoder.transform(kag_y_test)\n",
    "\n",
    "# Combined Data for cross validation/etc\n",
    "X = pd.concat([X_train, X_test], axis='index')\n",
    "y = pd.Series([*y_train, *y_test], index=X.index)\n",
    "```\n",
    "\n",
    "### A Boosted Model\n",
    "\n",
    "``` pycon\n",
    ">>> xg_oob = xgb.XGBClassifier()\n",
    ">>> xg_oob.fit(X_train, y_train)\n",
    ">>> xg_oob.score(X_test, y_test)\n",
    "0.7458563535911602\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> # Let's try w/ depth of 2 and 2 trees\n",
    ">>> xg2 = xgb.XGBClassifier(max_depth=2, n_estimators=2)\n",
    ">>> xg2.fit(X_train, y_train)\n",
    ">>> xg2.score(X_test, y_test)\n",
    "0.6685082872928176\n",
    "```\n",
    "\n",
    "``` python\n",
    "import dtreeviz\n",
    "\n",
    "viz = dtreeviz.model(xg2, X_train=X, y_train=y, target_name='Job',\n",
    "    feature_names=list(X_train.columns), \n",
    "    class_names=['DS', 'SE'], tree_index=0)\n",
    "viz.view(depth_range_to_display=[0,2])\n",
    "```\n",
    "\n",
    "### Understanding the Output of the Trees\n",
    "\n",
    "``` python\n",
    "xhelp.my_dot_export(xg2, num_trees=0, filename='img/xgb_md2.dot', \n",
    "                    title='First Tree') \n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> # Predicts 1 - Software engineer\n",
    ">>> se7894 = pd.DataFrame({'age': {7894: 22},                                            \n",
    "... 'education': {7894: 16.0},\n",
    "... 'years_exp': {7894: 1.0},\n",
    "... 'compensation': {7894: 0},\n",
    "... 'python': {7894: 1},\n",
    "... 'r': {7894: 0},\n",
    "... 'sql': {7894: 0},\n",
    "... 'Q1_Male': {7894: 1},                                   \n",
    "... 'Q1_Female': {7894: 0},\n",
    "... 'Q1_Prefer not to say': {7894: 0},\n",
    "... 'Q1_Prefer to self-describe': {7894: 0},\n",
    "... 'Q3_United States of America': {7894: 0},\n",
    "... 'Q3_India': {7894: 1},\n",
    "... 'Q3_China': {7894: 0},\n",
    "... 'major_cs': {7894: 0},\n",
    "... 'major_other': {7894: 0},\n",
    "... 'major_eng': {7894: 0},\n",
    "... 'major_stat': {7894: 0}})\n",
    ">>> xg2.predict_proba(se7894)\n",
    "array([[0.4986236, 0.5013764]], dtype=float32)\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> # Predicts 1 - Software engineer\n",
    ">>> xg2.predict(pd.DataFrame(se7894))\n",
    "array([1])\n",
    "```\n",
    "\n",
    "``` python\n",
    "xhelp.my_dot_export(xg2, num_trees=1, filename='img/xgb_md2_tree1.dot', title='Second Tree') \n",
    "```\n",
    "\n",
    "``` python\n",
    "def inv_logit(p):\n",
    "    return np.exp(p) / (1 + np.exp(p))\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> inv_logit(-0.08476+0.0902701)\n",
    "0.5013775215147345\n",
    "```\n",
    "\n",
    "### Summary\n",
    "\n",
    "### Exercises\n",
    "\n",
    "## Early Stopping\n",
    "\n",
    "### Early Stopping Rounds\n",
    "\n",
    "``` pycon\n",
    ">>> # Defaults\n",
    ">>> xg = xgb.XGBClassifier()\n",
    ">>> xg.fit(X_train, y_train)\n",
    ">>> xg.score(X_test, y_test)\n",
    "0.7458563535911602\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> xg = xgb.XGBClassifier(early_stopping_rounds=20)\n",
    ">>> xg.fit(X_train, y_train,\n",
    "...        eval_set=[(X_train, y_train),\n",
    "...                  (X_test, y_test)\n",
    "...                 ]\n",
    "...       )\n",
    ">>> xg.score(X_test, y_test)\n",
    "[0] validation_0-logloss:0.61534    validation_1-logloss:0.61775\n",
    "[1] validation_0-logloss:0.57046    validation_1-logloss:0.57623\n",
    "[2] validation_0-logloss:0.54011    validation_1-logloss:0.55333\n",
    "[3] validation_0-logloss:0.51965    validation_1-logloss:0.53711\n",
    "[4] validation_0-logloss:0.50419    validation_1-logloss:0.52511\n",
    "[5] validation_0-logloss:0.49176    validation_1-logloss:0.51741\n",
    "[6] validation_0-logloss:0.48159    validation_1-logloss:0.51277\n",
    "[7] validation_0-logloss:0.47221    validation_1-logloss:0.51040\n",
    "[8] validation_0-logloss:0.46221    validation_1-logloss:0.50713\n",
    "[9] validation_0-logloss:0.45700    validation_1-logloss:0.50583\n",
    "[10]    validation_0-logloss:0.45062    validation_1-logloss:0.50430\n",
    "[11]    validation_0-logloss:0.44533    validation_1-logloss:0.50338\n",
    "[12]    validation_0-logloss:0.43736    validation_1-logloss:0.50033\n",
    "[13]    validation_0-logloss:0.43399    validation_1-logloss:0.50034\n",
    "[14]    validation_0-logloss:0.43004    validation_1-logloss:0.50192\n",
    "[15]    validation_0-logloss:0.42550    validation_1-logloss:0.50268\n",
    "[16]    validation_0-logloss:0.42169    validation_1-logloss:0.50196\n",
    "[17]    validation_0-logloss:0.41854    validation_1-logloss:0.50223\n",
    "[18]    validation_0-logloss:0.41485    validation_1-logloss:0.50360\n",
    "[19]    validation_0-logloss:0.41228    validation_1-logloss:0.50527\n",
    "[20]    validation_0-logloss:0.40872    validation_1-logloss:0.50839\n",
    "[21]    validation_0-logloss:0.40490    validation_1-logloss:0.50623\n",
    "[22]    validation_0-logloss:0.40280    validation_1-logloss:0.50806\n",
    "[23]    validation_0-logloss:0.39942    validation_1-logloss:0.51007\n",
    "[24]    validation_0-logloss:0.39807    validation_1-logloss:0.50987\n",
    "[25]    validation_0-logloss:0.39473    validation_1-logloss:0.51189\n",
    "[26]    validation_0-logloss:0.39389    validation_1-logloss:0.51170\n",
    "[27]    validation_0-logloss:0.39040    validation_1-logloss:0.51218\n",
    "[28]    validation_0-logloss:0.38837    validation_1-logloss:0.51135\n",
    "[29]    validation_0-logloss:0.38569    validation_1-logloss:0.51202\n",
    "[30]    validation_0-logloss:0.37945    validation_1-logloss:0.51352\n",
    "[31]    validation_0-logloss:0.37840    validation_1-logloss:0.51545\n",
    "0.7558011049723757\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> xg.best_ntree_limit\n",
    "13\n",
    "```\n",
    "\n",
    "### Plotting Tree Performance\n",
    "\n",
    "``` pycon\n",
    ">>> # validation_0 is for training data\n",
    ">>> # validation_1 is for testing data\n",
    ">>> results = xg.evals_result()\n",
    ">>> results\n",
    "{'validation_0': OrderedDict([('logloss',\n",
    "               [0.6153406503923696,\n",
    "                0.5704566627034644,\n",
    "                0.5401074953836288,\n",
    "                0.519646179894983,\n",
    "                0.5041859194071372,\n",
    "                0.49175883369140716,\n",
    "                0.4815858465553177,\n",
    "                0.4722135672319274,\n",
    "                0.46221246084118905,\n",
    "                0.4570046103131291,\n",
    "                0.45062119092139025,\n",
    "                0.44533101600634545,\n",
    "                0.4373589513231934,\n",
    "                0.4339914069003403,\n",
    "                0.4300442738158372,\n",
    "                0.42550266018419824,\n",
    "                0.42168949383456633,\n",
    "                0.41853931894949614,\n",
    "                0.41485192559138645,\n",
    "                0.4122836278413833,\n",
    "                0.4087179538231096,\n",
    "                0.404898268053467,\n",
    "                0.4027963532207719,\n",
    "                0.39941699938733854,\n",
    "                0.3980718078477953,\n",
    "                0.39473153180519993,\n",
    "                0.39388538948800944,\n",
    "                0.39039599470886893,\n",
    "                0.38837148147752126,\n",
    "                0.38569152626668,\n",
    "                0.3794510693344513,\n",
    "                0.37840359436957194,\n",
    "                0.37538466192241227])]),\n",
    " 'validation_1': OrderedDict([('logloss',\n",
    "               [0.6177459120091813,\n",
    "                0.5762297115602546,\n",
    "                0.5533292921537852,\n",
    "                0.5371078260695736,\n",
    "                0.5251118483299708,\n",
    "                0.5174100387491574,\n",
    "                0.5127666981510036,\n",
    "                0.5103968678752362,\n",
    "                0.5071349115538004,\n",
    "                0.5058257413585542,\n",
    "                0.5043005662687247,\n",
    "                0.5033770955193438,\n",
    "                0.5003349146419797,\n",
    "                0.5003436393562437,\n",
    "                0.5019165392779843,\n",
    "                0.502677517614806,\n",
    "                0.501961292550791,\n",
    "                0.5022262006329157,\n",
    "                0.5035970173261607,\n",
    "                0.5052709663297096,\n",
    "                0.508388655664636,\n",
    "                0.5062287504923689,\n",
    "                0.5080608455824424,\n",
    "                0.5100736726054829,\n",
    "                0.5098673969229365,\n",
    "                0.5118910041889845,\n",
    "                0.5117007332982608,\n",
    "                0.5121825202836434,\n",
    "                0.5113475993625531,\n",
    "                0.5120185821281118,\n",
    "                0.5135189292720874,\n",
    "                0.5154504034915188,\n",
    "                0.5158137131755071])])}\n",
    "```\n",
    "\n",
    "``` python\n",
    "# Testing score is best at 13 trees\n",
    "results = xg.evals_result()\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax = (pd.DataFrame({'training': results['validation_0']['logloss'],\n",
    "                    'testing': results['validation_1']['logloss']})\n",
    "  .assign(ntrees=lambda adf: range(1, len(adf)+1))      \n",
    "  .set_index('ntrees')\n",
    "  .plot(figsize=(5,4), ax=ax, \n",
    "        title='eval_results with early_stopping')\n",
    ")\n",
    "ax.annotate('Best number \\nof trees (13)', xy=(13, .498),\n",
    "           xytext=(20,.42), arrowprops={'color':'k'})\n",
    "ax.set_xlabel('ntrees')\n",
    "```\n",
    "\n",
    "``` python\n",
    "# Using value from early stopping gives same result\n",
    ">>> xg13 = xgb.XGBClassifier(n_estimators=13)\n",
    ">>> xg13.fit(X_train, y_train,\n",
    "...          eval_set=[(X_train, y_train),\n",
    "...                    (X_test, y_test)]\n",
    "... )\n",
    ">>> xg13.score(X_test, y_test)\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> xg.score(X_test, y_test)\n",
    "0.7558011049723757\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> # No early stopping, uses all estimators\n",
    ">>> xg_no_es = xgb.XGBClassifier()\n",
    ">>> xg_no_es.fit(X_train, y_train)\n",
    ">>> xg_no_es.score(X_test, y_test)\n",
    "0.7458563535911602\n",
    "```\n",
    "\n",
    "### Different `eval_metrics`\n",
    "\n",
    "``` pycon\n",
    ">>> xg_err = xgb.XGBClassifier(early_stopping_rounds=20, \n",
    "...                            eval_metric='error')\n",
    ">>> xg_err.fit(X_train, y_train,\n",
    "...        eval_set=[(X_train, y_train),\n",
    "...                  (X_test, y_test)\n",
    "...                 ]\n",
    "...       )\n",
    ">>> xg_err.score(X_test, y_test)\n",
    "[0] validation_0-error:0.24739  validation_1-error:0.27072\n",
    "[1] validation_0-error:0.24218  validation_1-error:0.26188\n",
    "[2] validation_0-error:0.23839  validation_1-error:0.24751\n",
    "[3] validation_0-error:0.23697  validation_1-error:0.25193\n",
    "[4] validation_0-error:0.23081  validation_1-error:0.24530\n",
    "[5] validation_0-error:0.22607  validation_1-error:0.24420\n",
    "[6] validation_0-error:0.22180  validation_1-error:0.24862\n",
    "[7] validation_0-error:0.21801  validation_1-error:0.24862\n",
    "[8] validation_0-error:0.21280  validation_1-error:0.25304\n",
    "[9] validation_0-error:0.21043  validation_1-error:0.25304\n",
    "[10]    validation_0-error:0.20806  validation_1-error:0.24641\n",
    "[11]    validation_0-error:0.20284  validation_1-error:0.25193\n",
    "[12]    validation_0-error:0.20047  validation_1-error:0.24420\n",
    "[13]    validation_0-error:0.19668  validation_1-error:0.24420\n",
    "[14]    validation_0-error:0.19384  validation_1-error:0.24530\n",
    "[15]    validation_0-error:0.18815  validation_1-error:0.24199\n",
    "[16]    validation_0-error:0.18531  validation_1-error:0.24199\n",
    "[17]    validation_0-error:0.18389  validation_1-error:0.23867\n",
    "[18]    validation_0-error:0.18531  validation_1-error:0.23757\n",
    "[19]    validation_0-error:0.18815  validation_1-error:0.23867\n",
    "[20]    validation_0-error:0.18246  validation_1-error:0.24199\n",
    "[21]    validation_0-error:0.17915  validation_1-error:0.24862\n",
    "[22]    validation_0-error:0.17867  validation_1-error:0.24751\n",
    "[23]    validation_0-error:0.17630  validation_1-error:0.24199\n",
    "[24]    validation_0-error:0.17488  validation_1-error:0.24309\n",
    "[25]    validation_0-error:0.17251  validation_1-error:0.24530\n",
    "[26]    validation_0-error:0.17204  validation_1-error:0.24309\n",
    "[27]    validation_0-error:0.16825  validation_1-error:0.24199\n",
    "[28]    validation_0-error:0.16730  validation_1-error:0.24088\n",
    "[29]    validation_0-error:0.16019  validation_1-error:0.24199\n",
    "[30]    validation_0-error:0.15782  validation_1-error:0.24972\n",
    "[31]    validation_0-error:0.15972  validation_1-error:0.24862\n",
    "[32]    validation_0-error:0.15924  validation_1-error:0.24641\n",
    "[33]    validation_0-error:0.15403  validation_1-error:0.25635\n",
    "[34]    validation_0-error:0.15261  validation_1-error:0.25525\n",
    "[35]    validation_0-error:0.15213  validation_1-error:0.25525\n",
    "[36]    validation_0-error:0.15166  validation_1-error:0.25525\n",
    "[37]    validation_0-error:0.14550  validation_1-error:0.25525\n",
    "[38]    validation_0-error:0.14597  validation_1-error:0.25083\n",
    "0.7624309392265194\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> xg_err.best_ntree_limit\n",
    "19\n",
    "```\n",
    "\n",
    "### Summary\n",
    "\n",
    "### Exercises\n",
    "\n",
    "## XGBoost Hyperparameters\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "### Examining Hyperparameters\n",
    "\n",
    "``` pycon\n",
    ">>> xg = xgb.XGBClassifier() # set the hyperparamters in here\n",
    ">>> xg.fit(X_train, y_train)\n",
    ">>> xg.get_params()\n",
    "{'objective': 'binary:logistic',\n",
    " 'use_label_encoder': None,\n",
    " 'base_score': 0.5,\n",
    " 'booster': 'gbtree',\n",
    " 'callbacks': None,\n",
    " 'colsample_bylevel': 1,\n",
    " 'colsample_bynode': 1,\n",
    " 'colsample_bytree': 1,\n",
    " 'early_stopping_rounds': None,\n",
    " 'enable_categorical': False,\n",
    " 'eval_metric': None,\n",
    " 'feature_types': None,\n",
    " 'gamma': 0,\n",
    " 'gpu_id': -1,\n",
    " 'grow_policy': 'depthwise',\n",
    " 'importance_type': None,\n",
    " 'interaction_constraints': '',\n",
    " 'learning_rate': 0.300000012,\n",
    " 'max_bin': 256,\n",
    " 'max_cat_threshold': 64,\n",
    " 'max_cat_to_onehot': 4,\n",
    " 'max_delta_step': 0,\n",
    " 'max_depth': 6,\n",
    " 'max_leaves': 0,\n",
    " 'min_child_weight': 1,\n",
    " 'missing': nan,\n",
    " 'monotone_constraints': '()',\n",
    " 'n_estimators': 100,\n",
    " 'n_jobs': 0,\n",
    " 'num_parallel_tree': 1,\n",
    " 'predictor': 'auto',\n",
    " 'random_state': 0,\n",
    " 'reg_alpha': 0,\n",
    " 'reg_lambda': 1,\n",
    " 'sampling_method': 'uniform',\n",
    " 'scale_pos_weight': 1,\n",
    " 'subsample': 1,\n",
    " 'tree_method': 'exact',\n",
    " 'validate_parameters': 1,\n",
    " 'verbosity': None}\n",
    "```\n",
    "\n",
    "### Tuning Hyperparameters\n",
    "\n",
    "``` python\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ms.validation_curve(xgb.XGBClassifier(), X_train, y_train, param_name='gamma', \n",
    "    param_range=[0, .5, 1,5,10, 20, 30], n_jobs=-1, ax=ax)\n",
    "```\n",
    "\n",
    "### Intuitive Understanding of Learning Rate\n",
    "\n",
    "``` python\n",
    "# check impact of learning weight on scores\n",
    "xg_lr1 = xgb.XGBClassifier(learning_rate=1, max_depth=2)\n",
    "xg_lr1.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "``` python\n",
    "my_dot_export(xg_lr1, num_trees=0, filename='img/xg_depth2_tree0.dot', \n",
    "              title='Learning Rate set to 1')    \n",
    "```\n",
    "\n",
    "``` python\n",
    "# check impact of learning weight on scores\n",
    "xg_lr001 = xgb.XGBClassifier(learning_rate=.001, max_depth=2)\n",
    "xg_lr001.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "``` python\n",
    "my_dot_export(xg_lr001, num_trees=0, filename='img/xg_depth2_tree0_lr001.dot',\n",
    "              title='Learning Rate set to .001')    \n",
    "```\n",
    "\n",
    "### Grid Search\n",
    "\n",
    "``` python\n",
    "from sklearn import model_selection\n",
    "params = {'reg_lambda': [0],  # No effect\n",
    "          'learning_rate': [.1, .3], # makes each boost more conservative \n",
    "          'subsample': [.7, 1],\n",
    "          'max_depth': [2, 3],\n",
    "          'random_state': [42],\n",
    "          'n_jobs': [-1],\n",
    "          'n_estimators': [200]}\n",
    "\n",
    "xgb2 = xgb.XGBClassifier(early_stopping_rounds=5) \n",
    "cv = (model_selection.GridSearchCV(xgb2, params, cv=3, n_jobs=-1)\n",
    "    .fit(X_train, y_train,\n",
    "         eval_set=[(X_test, y_test)],\n",
    "         verbose=50\n",
    "    )\n",
    ")\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> cv.best_params_\n",
    "{'learning_rate': 0.3,\n",
    " 'max_depth': 2,\n",
    " 'n_estimators': 200,\n",
    " 'n_jobs': -1,\n",
    " 'random_state': 42,\n",
    " 'reg_lambda': 0,\n",
    " 'subsample': 1}\n",
    "```\n",
    "\n",
    "``` python\n",
    "params = {'learning_rate': 0.3,\n",
    "          'max_depth': 2,\n",
    "          'n_estimators': 200,\n",
    "          'n_jobs': -1,\n",
    "          'random_state': 42,\n",
    "          'reg_lambda': 0,\n",
    "          'subsample': 1\n",
    "}\n",
    "xgb_grid = xgb.XGBClassifier(**params, early_stopping_rounds=50)\n",
    "xgb_grid.fit(X_train, y_train, eval_set=[(X_train, y_train),\n",
    " (X_test, y_test)],\n",
    " verbose=10\n",
    ")\n",
    "```\n",
    "\n",
    "``` python\n",
    "# vs default\n",
    "xgb_def = xgb.XGBClassifier(early_stopping_rounds=50)\n",
    "xgb_def.fit(X_train, y_train, eval_set=[(X_train, y_train),\n",
    " (X_test, y_test)],\n",
    " verbose=10\n",
    ")\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> xgb_def.score(X_test, y_test), xgb_grid.score(X_test, y_test)\n",
    "(0.7558011049723757, 0.7524861878453039)\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> results_default = model_selection.cross_val_score(\n",
    "...    xgb.XGBClassifier(),\n",
    "...    X=X, y=y,\n",
    "...    cv=4\n",
    "... )\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> results_default\n",
    "array([0.71352785, 0.72413793, 0.69496021, 0.74501992])\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> results_default.mean()\n",
    "0.7194114787534214\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> results_grid = model_selection.cross_val_score(\n",
    "...    xgb.XGBClassifier(**params),\n",
    "...    X=X, y=y,\n",
    "...    cv=4\n",
    "... )\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> results_grid\n",
    "array([0.74137931, 0.74137931, 0.74801061, 0.73572377])\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> results_grid.mean()\n",
    "0.7416232505873941\n",
    "```\n",
    "\n",
    "### Summary\n",
    "\n",
    "### Exercises\n",
    "\n",
    "## Hyperopt\n",
    "\n",
    "### Bayesian Optimization\n",
    "\n",
    "### Exhaustive Tuning with Hyperopt\n",
    "\n",
    "``` python\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score  \n",
    "\n",
    "from typing import Any, Dict, Union\n",
    "\n",
    "def hyperparameter_tuning(space: Dict[str, Union[float, int]], \n",
    "                    X_train: pd.DataFrame, y_train: pd.Series, \n",
    "                    X_test: pd.DataFrame, y_test: pd.Series, \n",
    "                    early_stopping_rounds: int=50,\n",
    "                    metric:callable=accuracy_score) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Perform hyperparameter tuning for an XGBoost classifier.\n",
    "\n",
    "    This function takes a dictionary of hyperparameters, training \n",
    "    and test data, and an optional value for early stopping rounds, \n",
    "    and returns a dictionary with the loss and model resulting from \n",
    "    the tuning process. The model is trained using the training \n",
    "    data and evaluated on the test data. The loss is computed as \n",
    "    the negative of the accuracy score.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    space : Dict[str, Union[float, int]]\n",
    "        A dictionary of hyperparameters for the XGBoost classifier.\n",
    "    X_train : pd.DataFrame\n",
    "        The training data.\n",
    "    y_train : pd.Series\n",
    "        The training target.\n",
    "    X_test : pd.DataFrame\n",
    "        The test data.\n",
    "    y_test : pd.Series\n",
    "        The test target.\n",
    "    early_stopping_rounds : int, optional\n",
    "        The number of early stopping rounds to use. The default value \n",
    "        is 50.\n",
    "    metric : callable\n",
    "        Metric to maximize. Default is accuracy\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, Any]\n",
    "        A dictionary with the loss and model resulting from the \n",
    "        tuning process. The loss is a float, and the model is an \n",
    "        XGBoost classifier.\n",
    "    \"\"\"\n",
    "    int_vals = ['max_depth', 'reg_alpha']\n",
    "    space = {k: (int(val) if k in int_vals else val)\n",
    "             for k,val in space.items()}\n",
    "    space['early_stopping_rounds'] = early_stopping_rounds\n",
    "    model = xgb.XGBClassifier(**space)\n",
    "    evaluation = [(X_train, y_train),\n",
    "                  (X_test, y_test)]\n",
    "    model.fit(X_train, y_train,\n",
    "              eval_set=evaluation, \n",
    "              verbose=False)    \n",
    "         \n",
    "    pred = model.predict(X_test)\n",
    "    score = metric(y_test, pred)\n",
    "    return {'loss': -score, 'status': STATUS_OK, 'model': model}\n",
    "```\n",
    "\n",
    "``` python\n",
    "options = {'max_depth': hp.quniform('max_depth', 1, 8, 1),  # tree\n",
    "    'min_child_weight': hp.loguniform('min_child_weight', -2, 3),\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1),   # stochastic\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0, 10),\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 1, 10),\n",
    "    'gamma': hp.loguniform('gamma', -10, 10), # regularization\n",
    "    'learning_rate': hp.loguniform('learning_rate', -7, 0),  # boosting\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn=lambda space: hyperparameter_tuning(space, X_train, y_train, \n",
    "                                                   X_test, y_test),            \n",
    "    space=options,           \n",
    "    algo=tpe.suggest,            \n",
    "    max_evals=2_000,            \n",
    "    trials=trials,\n",
    "    #timeout=60*5 # 5 minutes\n",
    ")\n",
    "```\n",
    "\n",
    "``` python\n",
    "# 2 hours of training (paste best in here)\n",
    "long_params = {'colsample_bytree': 0.6874845219014455, \n",
    "               'gamma': 0.06936323554883501, \n",
    "               'learning_rate': 0.21439214284976907, \n",
    "               'max_depth': 6, \n",
    "               'min_child_weight': 0.6678357091609912, \n",
    "               'reg_alpha': 3.2979862933185546, \n",
    "               'reg_lambda': 7.850943400390477, \n",
    "               'subsample': 0.999767483950891}\n",
    "```\n",
    "\n",
    "``` python\n",
    "xg_ex = xgb.XGBClassifier(**long_params, early_stopping_rounds=50,\n",
    "                            n_estimators=500)\n",
    "xg_ex.fit(X_train, y_train,\n",
    "       eval_set=[(X_train, y_train),\n",
    "                 (X_test, y_test)\n",
    "                ],\n",
    "        verbose=100\n",
    "      )\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> xg_ex.score(X_test, y_test)\n",
    "0.7580110497237569\n",
    "```\n",
    "\n",
    "### Defining Parameter Distributions\n",
    "\n",
    "``` pycon\n",
    ">>> from hyperopt import hp, pyll\n",
    ">>> pyll.stochastic.sample(hp.choice('value', ['a', 'b', 'c']))\n",
    "'a'\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> pyll.stochastic.sample(hp.pchoice('value', [(.05, 'a'), (.9, 'b'), \n",
    "...     (.05, 'c')]))\n",
    "'c'\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> from hyperopt import hp, pyll\n",
    "\n",
    ">>> pyll.stochastic.sample(hp.uniform('value', 0, 1))\n",
    "0.7875384438202859\n",
    "```\n",
    "\n",
    "``` python\n",
    "uniform_vals = [pyll.stochastic.sample(hp.uniform('value', 0, 1)) \n",
    "               for _ in range(10_000)]\n",
    "```\n",
    "\n",
    "``` python\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.hist(uniform_vals)\n",
    "```\n",
    "\n",
    "``` python\n",
    "loguniform_vals = [pyll.stochastic.sample(hp.loguniform('value', -5, 5)) \n",
    "               for _ in range(10_000)]\n",
    "```\n",
    "\n",
    "``` python\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.hist(loguniform_vals)\n",
    "```\n",
    "\n",
    "``` python\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "(pd.Series(np.arange(-5, 5, step=.1))\n",
    " .rename('x')\n",
    " .to_frame()\n",
    " .assign(y=lambda adf:np.exp(adf.x))\n",
    " .plot(x='x', y='y', ax=ax)\n",
    ")\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> from hyperopt import hp, pyll\n",
    ">>> from math import log\n",
    ">>> pyll.stochastic.sample(hp.loguniform('value', log(.1), log(10)))\n",
    "3.0090767867889174\n",
    "```\n",
    "\n",
    "``` python\n",
    "quniform_vals = [pyll.stochastic.sample(hp.quniform('value', -5, 5, q=2)) \n",
    "               for _ in range(10_000)]\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> pd.Series(quniform_vals).value_counts()\n",
    "-0.0    2042\n",
    "-2.0    2021\n",
    " 2.0    2001\n",
    " 4.0    2000\n",
    "-4.0    1936\n",
    "dtype: int64\n",
    "```\n",
    "\n",
    "### Exploring the Trials\n",
    "\n",
    "``` python\n",
    "from typing import Any, Dict, Sequence\n",
    "def trial2df(trial: Sequence[Dict[str, Any]]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert a Trial object (sequence of trial dictionaries)\n",
    "    to a Pandas DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    trial : List[Dict[str, Any]]\n",
    "        A list of trial dictionaries.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A DataFrame with columns for the loss, trial id, and\n",
    "        values from each trial dictionary.\n",
    "    \"\"\"\n",
    "    vals = []\n",
    "    for t in trial:\n",
    "        result = t['result']\n",
    "        misc = t['misc']\n",
    "        val = {k:(v[0] if isinstance(v, list) else v)  \n",
    "               for k,v in misc['vals'].items()\n",
    "              }\n",
    "        val['loss'] = result['loss']\n",
    "        val['tid'] = t['tid']\n",
    "        vals.append(val)\n",
    "    return pd.DataFrame(vals)\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> hyper2hr = trial2df(trials)\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> hyper2hr\n",
    "      colsample_bytree       gamma  learning_rate  ...  subsample      loss  \\\n",
    "0             0.854670    2.753933       0.042056  ...   0.913247 -0.744751   \n",
    "1             0.512653    0.153628       0.611973  ...   0.550048 -0.746961   \n",
    "2             0.552569    1.010561       0.002412  ...   0.508593 -0.735912   \n",
    "3             0.604020  682.836185       0.005037  ...   0.536935 -0.545856   \n",
    "4             0.785281    0.004130       0.015200  ...   0.691211 -0.739227   \n",
    "...                ...         ...            ...  ...        ...       ...   \n",
    "1995          0.717890    0.000543       0.141629  ...   0.893414 -0.765746   \n",
    "1996          0.725305    0.000248       0.172854  ...   0.919415 -0.765746   \n",
    "1997          0.698025    0.028484       0.162207  ...   0.952204 -0.770166   \n",
    "1998          0.688053    0.068223       0.099814  ...   0.939489 -0.762431   \n",
    "1999          0.666225    0.125253       0.203441  ...   0.980354 -0.767956   \n",
    "\n",
    "       tid  \n",
    "0        0  \n",
    "1        1  \n",
    "2        2  \n",
    "3        3  \n",
    "4        4  \n",
    "...    ...  \n",
    "1995  1995  \n",
    "1996  1996  \n",
    "1997  1997  \n",
    "1998  1998  \n",
    "1999  1999  \n",
    "\n",
    "[2000 rows x 10 columns]\n",
    "```\n",
    "\n",
    "``` python\n",
    "import seaborn as sns\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "sns.heatmap(hyper2hr.corr(method='spearman'),\n",
    "    cmap='RdBu', annot=True, fmt='.2f', vmin=-1, vmax=1, ax=ax\n",
    ")\n",
    "```\n",
    "\n",
    "``` python\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "(hyper2hr\n",
    "  .plot.scatter(x='tid', y='loss', alpha=.1, color='purple', ax=ax)\n",
    ")\n",
    "```\n",
    "\n",
    "``` python\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "(hyper2hr\n",
    "  .plot.scatter(x='max_depth', y='loss', alpha=1, color='purple', ax=ax)\n",
    ")\n",
    "```\n",
    "\n",
    "``` python\n",
    "import numpy as np\n",
    "\n",
    "def jitter(df: pd.DataFrame, col: str, amount: float=1) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Add random noise to the values in a Pandas DataFrame column.\n",
    "\n",
    "    This function adds random noise to the values in a specified \n",
    "    column of a Pandas DataFrame. The noise is uniform random \n",
    "    noise with a range of `amount` centered around zero. The \n",
    "    function returns a Pandas Series with the jittered values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The input DataFrame.\n",
    "    col : str\n",
    "        The name of the column to jitter.\n",
    "    amount : float, optional\n",
    "        The range of the noise to add. The default value is 1.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        A Pandas Series with the jittered values.\n",
    "    \"\"\"\n",
    "    vals = np.random.uniform(low=-amount/2, high=amount/2,\n",
    "                            size=df.shape[0])\n",
    "    return df[col] + vals\n",
    "```\n",
    "\n",
    "``` python\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "(hyper2hr\n",
    "  .assign(max_depth=lambda df:jitter(df, 'max_depth', amount=.8))\n",
    "  .plot.scatter(x='max_depth', y='loss', alpha=.1, color='purple', ax=ax)\n",
    ")\n",
    "```\n",
    "\n",
    "``` python\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "(hyper2hr\n",
    "  .assign(max_depth=lambda df:jitter(df, 'max_depth', amount=.8))\n",
    "  .plot.scatter(x='max_depth', y='loss', alpha=.5, \n",
    "               color='tid', cmap='viridis', ax=ax)\n",
    ")\n",
    "```\n",
    "\n",
    "``` python\n",
    "import seaborn as sns\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "sns.violinplot(x='max_depth', y='loss', data=hyper2hr, kind='violin', ax=ax)\n",
    "```\n",
    "\n",
    "``` python\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "(hyper2hr\n",
    "  .plot.scatter(x='reg_alpha', y='colsample_bytree', alpha=.8,\n",
    "               color='tid', cmap='viridis', ax=ax)\n",
    ")\n",
    "\n",
    "ax.annotate('Min Loss (-0.77)', xy=(4.56, 0.692),\n",
    "           xytext=(.7, .84), arrowprops={'color':'k'})\n",
    "```\n",
    "\n",
    "``` python\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "(hyper2hr\n",
    "  .plot.scatter(x='gamma', y='loss', alpha=.1, color='purple', ax=ax)\n",
    ")\n",
    "```\n",
    "\n",
    "``` python\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "(hyper2hr\n",
    "  .plot.scatter(x='gamma', y='loss', alpha=.5, color='tid', ax=ax, \n",
    "                logx=True, cmap='viridis')\n",
    ")\n",
    "\n",
    "ax.annotate('Min Loss (-0.77)', xy=(0.000581, -0.777),\n",
    "           xytext=(1, -.6), arrowprops={'color':'k'})\n",
    "```\n",
    "\n",
    "### EDA with Plotly\n",
    "\n",
    "``` python\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "def plot_3d_mesh(df: pd.DataFrame, x_col: str, y_col: str, \n",
    "                 z_col: str) -> go.Figure:\n",
    "    \"\"\"\n",
    "    Create a 3D mesh plot using Plotly.\n",
    "\n",
    "    This function creates a 3D mesh plot using Plotly, with \n",
    "    the `x_col`, `y_col`, and `z_col` columns of the `df` \n",
    "    DataFrame as the x, y, and z values, respectively. The \n",
    "    plot has a title and axis labels that match the column \n",
    "    names, and the intensity of the mesh is proportional \n",
    "    to the values in the `z_col` column. The function returns \n",
    "    a Plotly Figure object that can be displayed or saved as \n",
    "    desired.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The DataFrame containing the data to plot.\n",
    "    x_col : str\n",
    "        The name of the column to use as the x values.\n",
    "    y_col : str\n",
    "        The name of the column to use as the y values.\n",
    "    z_col : str\n",
    "        The name of the column to use as the z values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    go.Figure\n",
    "        A Plotly Figure object with the 3D mesh plot.\n",
    "    \"\"\"\n",
    "    fig = go.Figure(data=[go.Mesh3d(x=df[x_col], y=df[y_col], z=df[z_col],\n",
    "        intensity=df[z_col]/ df[z_col].min(),\n",
    "        hovertemplate=f\"{z_col}: %{{z}}<br>{x_col}: %{{x}}<br>{y_col}: \"\n",
    "                                    \"%{{y}}<extra></extra>\")],\n",
    "    )\n",
    "\n",
    "    fig.update_layout( \n",
    "        title=dict(text=f'{y_col} vs {x_col}'),\n",
    "        scene = dict(\n",
    "          xaxis_title=x_col,\n",
    "          yaxis_title=y_col,\n",
    "          zaxis_title=z_col),\n",
    "        width=700,\n",
    "        margin=dict(r=20, b=10, l=10, t=50)\n",
    "    )\n",
    "    return fig\n",
    "```\n",
    "\n",
    "``` python\n",
    "fig = plot_3d_mesh(hyper2hr.query('gamma < .2'),\n",
    "    'reg_lambda', 'gamma', 'loss')\n",
    "           \n",
    "fig\n",
    "```\n",
    "\n",
    "``` python\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "def plot_3d_scatter(df: pd.DataFrame, x_col: str, y_col: str, \n",
    "                 z_col: str, color_col: str, \n",
    "                 opacity: float=1) -> go.Figure:\n",
    "    \"\"\"\n",
    "    Create a 3D scatter plot using Plotly Express.\n",
    "\n",
    "    This function creates a 3D scatter plot using Plotly Express, \n",
    "    with the `x_col`, `y_col`, and `z_col` columns of the `df` \n",
    "    DataFrame as the x, y, and z values, respectively. The points \n",
    "    in the plot are colored according to the values in the \n",
    "    `color_col` column, using a continuous color scale. The \n",
    "    function returns a Plotly Express scatter_3d object that \n",
    "    can be displayed or saved as desired.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The DataFrame containing the data to plot.\n",
    "    x_col : str\n",
    "        The name of the column to use as the x values.\n",
    "    y_col : str\n",
    "        The name of the column to use as the y values.\n",
    "    z_col : str\n",
    "        The name of the column to use as the z values.\n",
    "    color_col : str\n",
    "        The name of the column to use for coloring.\n",
    "    opacity : float\n",
    "        The opacity (alpha) of the points.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    go.Figure\n",
    "        A Plotly Figure object with the 3D mesh plot.\n",
    "    \"\"\"\n",
    "    fig = px.scatter_3d(data_frame=df, x=x_col,\n",
    "                y=y_col, z=z_col, color=color_col,\n",
    "                color_continuous_scale=px.colors.sequential.Viridis_r,\n",
    "                opacity=opacity)\n",
    "    return fig\n",
    "```\n",
    "\n",
    "``` python\n",
    "plot_3d_scatter(hyper2hr.query('gamma < .2'), \n",
    "              'reg_lambda', 'gamma', 'tid', color_col='loss')\n",
    "```\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "### Exercises\n",
    "\n",
    "## Step-wise Tuning with Hyperopt\n",
    "\n",
    "### Groups of Hyperparameters\n",
    "\n",
    "``` python\n",
    "from hyperopt import fmin, tpe, hp, Trials\n",
    "params = {'random_state': 42}\n",
    "\n",
    "rounds = [{'max_depth': hp.quniform('max_depth', 1, 8, 1),  # tree\n",
    "           'min_child_weight': hp.loguniform('min_child_weight', -2, 3)},\n",
    "          {'subsample': hp.uniform('subsample', 0.5, 1),   # stochastic\n",
    "           'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1)},\n",
    "          {'reg_alpha': hp.uniform('reg_alpha', 0, 10),\n",
    "            'reg_lambda': hp.uniform('reg_lambda', 1, 10),},\n",
    "          {'gamma': hp.loguniform('gamma', -10, 10)}, # regularization\n",
    "          {'learning_rate': hp.loguniform('learning_rate', -7, 0)} # boosting\n",
    "]\n",
    "\n",
    "all_trials = []\n",
    "for round in rounds:\n",
    "    params = {**params, **round}\n",
    "    trials = Trials()\n",
    "    best = fmin(fn=lambda space: xhelp.hyperparameter_tuning(space, X_train, \n",
    "                                        y_train, X_test, y_test),            \n",
    "        space=params,           \n",
    "        algo=tpe.suggest,            \n",
    "        max_evals=20,            \n",
    "        trials=trials,\n",
    "    )\n",
    "    params = {**params, **best}\n",
    "    all_trials.append(trials)\n",
    "```\n",
    "\n",
    "### Visualization Hyperparameter Scores\n",
    "\n",
    "``` python\n",
    "xhelp.plot_3d_mesh(xhelp.trial2df(all_trials[2]),\n",
    "    'reg_alpha', 'reg_lambda', 'loss')    \n",
    "```\n",
    "\n",
    "### Training an Optimized Model\n",
    "\n",
    "``` python\n",
    "step_params = {'random_state': 42,\n",
    " 'max_depth': 5,\n",
    " 'min_child_weight': 0.6411044640540848,\n",
    " 'subsample': 0.9492383155577023,\n",
    " 'colsample_bytree': 0.6235721099295888,\n",
    " 'gamma': 0.00011273797329538491,\n",
    " 'learning_rate': 0.24399020050740935}\n",
    "```\n",
    "\n",
    "``` python\n",
    "xg_step = xgb.XGBClassifier(**step_params, early_stopping_rounds=50,\n",
    "                            n_estimators=500)\n",
    "xg_step.fit(X_train, y_train,\n",
    "       eval_set=[(X_train, y_train),\n",
    "                 (X_test, y_test)\n",
    "                ],\n",
    "        verbose=100\n",
    "      )\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> xg_step.score(X_test, y_test)\n",
    "0.7613259668508288\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> xg_def = xgb.XGBClassifier()\n",
    ">>> xg_def.fit(X_train, y_train)\n",
    ">>> xg_def.score(X_test, y_test)\n",
    "0.7458563535911602\n",
    "```\n",
    "\n",
    "### Summary\n",
    "\n",
    "### Exercises\n",
    "\n",
    "## Do you have enough data?\n",
    "\n",
    "### Learning Curves\n",
    "\n",
    "``` python\n",
    "params = {'learning_rate': 0.3,\n",
    " 'max_depth': 2,\n",
    " 'n_estimators': 200,\n",
    " 'n_jobs': -1,\n",
    " 'random_state': 42,\n",
    " 'reg_lambda': 0,\n",
    " 'subsample': 1}\n",
    "```\n",
    "\n",
    "``` python\n",
    "import yellowbrick.model_selection as ms\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "viz = ms.learning_curve(xgb.XGBClassifier(**params),\n",
    "      X, y, ax=ax\n",
    ")\n",
    "ax.set_ylim(0.6, 1)\n",
    "```\n",
    "\n",
    "### Learning Curves for Decision Trees\n",
    "\n",
    "``` python\n",
    "# tuned tree\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "viz = ms.learning_curve(tree.DecisionTreeClassifier(max_depth=7),\n",
    "      X, y, ax=ax)\n",
    "viz.ax.set_ylim(0.6, 1)\n",
    "```\n",
    "\n",
    "### Underfit Learning Curves\n",
    "\n",
    "``` python\n",
    "# underfit\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "viz = ms.learning_curve(tree.DecisionTreeClassifier(max_depth=1),\n",
    "      X, y, ax=ax\n",
    ")\n",
    "ax.set_ylim(0.6, 1)\n",
    "```\n",
    "\n",
    "### Overfit Learning Curves\n",
    "\n",
    "``` python\n",
    "# overfit\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "viz = ms.learning_curve(tree.DecisionTreeClassifier(),\n",
    "      X, y, ax=ax\n",
    ")\n",
    "ax.set_ylim(0.6, 1)\n",
    "```\n",
    "\n",
    "### Summary\n",
    "\n",
    "### Exercises\n",
    "\n",
    "## Model Evaluation\n",
    "\n",
    "### Accuracy\n",
    "\n",
    "``` python\n",
    "xgb_def = xgb.XGBClassifier()\n",
    "xgb_def.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> xgb_def.score(X_test, y_test)\n",
    "0.7458563535911602\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> from sklearn import metrics\n",
    ">>> metrics.accuracy_score(y_test, xgb_def.predict(X_test))\n",
    "0.7458563535911602\n",
    "```\n",
    "\n",
    "### Confusion Matrix\n",
    "\n",
    "``` python\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "classifier.confusion_matrix(xgb_def, X_train, y_train,\n",
    "                            X_test, y_test,\n",
    "                            classes=['DS', 'SE'], ax=ax\n",
    "                           )\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> from sklearn import metrics\n",
    ">>> cm = metrics.confusion_matrix(y_test, xgb_def.predict(X_test))\n",
    ">>> cm\n",
    "array([[372, 122],\n",
    "       [108, 303]])\n",
    "```\n",
    "\n",
    "``` python\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, \n",
    "                                      display_labels=['DS', 'SE'])\n",
    "disp.plot(ax=ax, cmap='Blues')\n",
    "```\n",
    "\n",
    "``` python\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "cm = metrics.confusion_matrix(y_test, xgb_def.predict(X_test), \n",
    "                             normalize='true')\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, \n",
    "                                      display_labels=['DS', 'SE'])                        \n",
    "disp.plot(ax=ax, cmap='Blues')\n",
    "```\n",
    "\n",
    "### Precision and Recall\n",
    "\n",
    "``` pycon\n",
    ">>> metrics.precision_score(y_test, xgb_def.predict(X_test))\n",
    "0.7129411764705882\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> metrics.recall_score(y_test, xgb_def.predict(X_test))\n",
    "0.7372262773722628\n",
    "```\n",
    "\n",
    "``` python\n",
    "from yellowbrick import classifier\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "classifier.precision_recall_curve(xgb_def, X_train, y_train,\n",
    "    X_test, y_test, micro=False, macro=False, ax=ax, per_class=True)\n",
    "ax.set_ylim((0,1.05))\n",
    "```\n",
    "\n",
    "### F1 Score\n",
    "\n",
    "``` pycon\n",
    ">>> metrics.f1_score(y_test, xgb_def.predict(X_test))\n",
    "0.7248803827751197\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> print(metrics.classification_report(y_test, \n",
    "...     y_pred=xgb_def.predict(X_test), target_names=['DS', 'SE']))\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          DS       0.78      0.75      0.76       494\n",
    "          SE       0.71      0.74      0.72       411\n",
    "\n",
    "    accuracy                           0.75       905\n",
    "   macro avg       0.74      0.75      0.74       905\n",
    "weighted avg       0.75      0.75      0.75       905\n",
    "```\n",
    "\n",
    "``` python\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "classifier.classification_report(xgb_def, X_train, y_train,\n",
    "    X_test, y_test, classes=['DS', 'SE'],\n",
    "    micro=False, macro=False, ax=ax)\n",
    "```\n",
    "\n",
    "### ROC Curve\n",
    "\n",
    "``` python\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "metrics.RocCurveDisplay.from_estimator(xgb_def,\n",
    "                       X_test, y_test,ax=ax, label='default')\n",
    "metrics.RocCurveDisplay.from_estimator(xg_step,\n",
    "                       X_test, y_test,ax=ax)\n",
    "```\n",
    "\n",
    "``` python\n",
    "fig, axes = plt.subplots(figsize=(8, 4), ncols=2)\n",
    "metrics.RocCurveDisplay.from_estimator(xgb_def,\n",
    "                       X_train, y_train,ax=axes[0], label='detault train')\n",
    "metrics.RocCurveDisplay.from_estimator(xgb_def,\n",
    "                       X_test, y_test,ax=axes[0])\n",
    "axes[0].set(title='ROC plots for default model')\n",
    "\n",
    "metrics.RocCurveDisplay.from_estimator(xg_step,\n",
    "                       X_train, y_train,ax=axes[1], label='step train')\n",
    "metrics.RocCurveDisplay.from_estimator(xg_step,\n",
    "                       X_test, y_test,ax=axes[1])\n",
    "axes[1].set(title='ROC plots for stepwise model')\n",
    "```\n",
    "\n",
    "### Threshold Metrics\n",
    "\n",
    "``` python\n",
    "class ThresholdXGBClassifier(xgb.XGBClassifier):\n",
    "    def __init__(self, threshold=0.5, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def predict(self, X, *args, **kwargs):\n",
    "        \"\"\"Predict with `threshold` applied to predicted class probabilities.\n",
    "        \"\"\"\n",
    "        proba = self.predict_proba(X, *args, **kwargs)\n",
    "        return (proba[:, 1] > self.threshold).astype(int)\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> xgb_def = xgb.XGBClassifier()\n",
    ">>> xgb_def.fit(X_train, y_train)\n",
    ">>> xgb_def.predict_proba(X_test.iloc[[0]])\n",
    "array([[0.14253652, 0.8574635 ]], dtype=float32)\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> xgb_def.predict(X_test.iloc[[0]])\n",
    "array([1])\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> xgb90 = ThresholdXGBClassifier(threshold=.9, verbosity=0)\n",
    ">>> xgb90.fit(X_train, y_train)\n",
    ">>> xgb90.predict(X_test.iloc[[0]])\n",
    "array([0])\n",
    "```\n",
    "\n",
    "``` python\n",
    "def get_tpr_fpr(probs, y_truth):\n",
    "    \"\"\"\n",
    "    Calculates true positive rate (TPR) and false positive rate\n",
    "    (FPR) given predicted probabilities and ground truth labels.\n",
    "\n",
    "    Parameters:\n",
    "    probs (np.array): predicted probabilities of positive class\n",
    "    y_truth (np.array): ground truth labels\n",
    "\n",
    "    Returns:\n",
    "    tuple: (tpr, fpr)\n",
    "    \"\"\"\n",
    "    tp = (probs == 1) & (y_truth == 1)\n",
    "    tn = (probs < 1) & (y_truth == 0)\n",
    "    fp = (probs == 1) & (y_truth == 0)\n",
    "    fn = (probs < 1) & (y_truth == 1)\n",
    "    tpr = tp.sum() / (tp.sum() + fn.sum())\n",
    "    fpr = fp.sum() / (fp.sum() + tn.sum())\n",
    "    return tpr, fpr\n",
    "\n",
    "\n",
    "vals = []\n",
    "for thresh in np.arange(0, 1, step=.05):\n",
    "    probs = xg_step.predict_proba(X_test)[:, 1]\n",
    "    tpr, fpr = get_tpr_fpr(probs > thresh, y_test)\n",
    "    val = [thresh, tpr, fpr]\n",
    "    for metric in [metrics.accuracy_score, metrics.precision_score,\n",
    "                   metrics.recall_score, metrics.f1_score, \n",
    "                   metrics.roc_auc_score]:\n",
    "        val.append(metric(y_test, probs > thresh))\n",
    "    vals.append(val)\n",
    "```\n",
    "\n",
    "``` python\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "(pd.DataFrame(vals, columns=['thresh', 'tpr/rec', 'fpr', 'acc', \n",
    "                             'prec', 'rec', 'f1', 'auc'])\n",
    " .drop(columns='rec')\n",
    " .set_index('thresh')\n",
    " .plot(ax=ax, title='Threshold Metrics')\n",
    ")\n",
    "```\n",
    "\n",
    "### Cumulative Gains Curve\n",
    "\n",
    "``` python\n",
    "import scikitplot\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "y_probs = xgb_def.predict_proba(X_test)\n",
    "scikitplot.metrics.plot_cumulative_gain(y_test, y_probs, ax=ax)\n",
    "ax.plot([0, (y_test == 1).mean(), 1], [0, 1, 1], label='Optimal Class 1')\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.annotate('Reach 60% of\\nClass 1\\nby contacting top 35%', xy=(.35, .6),\n",
    "           xytext=(.55,.25), arrowprops={'color':'k'})\n",
    "ax.legend()\n",
    "```\n",
    "\n",
    "### Lift Curves\n",
    "\n",
    "``` python\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "y_probs = xgb_def.predict_proba(X_test)\n",
    "scikitplot.metrics.plot_lift_curve(y_test, y_probs, ax=ax)\n",
    "mean = (y_test == 1).mean()\n",
    "ax.plot([0, mean, 1], [1/mean, 1/mean, 1], label='Optimal Class 1')\n",
    "ax.legend()\n",
    "```\n",
    "\n",
    "### Summary\n",
    "\n",
    "### Exercises\n",
    "\n",
    "## Training For Different Metrics\n",
    "\n",
    "### Metric overview\n",
    "\n",
    "### Training with Validation Curves\n",
    "\n",
    "``` python\n",
    "from yellowbrick import model_selection as ms\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ms.validation_curve(xgb.XGBClassifier(), X_train, y_train,\n",
    "    scoring='accuracy', param_name='learning_rate', \n",
    "    param_range=[0.001, .01, .05, .1, .2, .5, .9, 1], ax=ax\n",
    ")\n",
    "ax.set_xlabel('Accuracy')\n",
    "```\n",
    "\n",
    "``` python\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ms.validation_curve(xgb.XGBClassifier(), X_train, y_train,\n",
    "    scoring='roc_auc', param_name='learning_rate',\n",
    "    param_range=[0.001, .01, .05, .1, .2, .5, .9, 1], ax=ax\n",
    "    )\n",
    "ax.set_xlabel('roc_auc')\n",
    "```\n",
    "\n",
    "### Step-wise Recall Tuning\n",
    "\n",
    "``` python\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from hyperopt import hp, Trials, fmin, tpe\n",
    "params = {'random_state': 42}\n",
    "\n",
    "rounds = [{'max_depth': hp.quniform('max_depth', 1, 9, 1),  # tree\n",
    "           'min_child_weight': hp.loguniform('min_child_weight', -2, 3)},\n",
    "          {'subsample': hp.uniform('subsample', 0.5, 1),   # stochastic\n",
    "           'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1)},\n",
    "          {'gamma': hp.loguniform('gamma', -10, 10)}, # regularization\n",
    "          {'learning_rate': hp.loguniform('learning_rate', -7, 0)} # boosting\n",
    "]\n",
    "\n",
    "for round in rounds:\n",
    "    params = {**params, **round}\n",
    "    trials = Trials()\n",
    "    best = fmin(fn=lambda space: xhelp.hyperparameter_tuning(\n",
    "        space, X_train, y_train, X_test, y_test, metric=roc_auc_score),\n",
    "        space=params,           \n",
    "        algo=tpe.suggest,            \n",
    "        max_evals=40,            \n",
    "        trials=trials,\n",
    "    )\n",
    "    params = {**params, **best}\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> xgb_def = xgb.XGBClassifier()\n",
    ">>> xgb_def.fit(X_train, y_train)\n",
    ">>> metrics.roc_auc_score(y_test, xgb_def.predict(X_test))\n",
    "0.7451313573096131\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> # the values from above training\n",
    ">>> params = {'random_state': 42,\n",
    "...  'max_depth': 4,\n",
    "...  'min_child_weight': 4.808561584650579,\n",
    "...  'subsample': 0.9265505972233746,\n",
    "...  'colsample_bytree': 0.9870944989347749,\n",
    "...  'gamma': 0.1383762861356536,\n",
    "...  'learning_rate': 0.13664139307301595}\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> xgb_tuned = xgb.XGBClassifier(**params, early_stopping_rounds=50,\n",
    "...    n_estimators=500)\n",
    ">>> xgb_tuned.fit(X_train, y_train, eval_set=[(X_train, y_train), \n",
    "...     (X_test, y_test)], verbose=100)\n",
    "[0] validation_0-logloss:0.66207    validation_1-logloss:0.66289\n",
    "[100]   validation_0-logloss:0.44945    validation_1-logloss:0.49416\n",
    "[150]   validation_0-logloss:0.43196    validation_1-logloss:0.49833\n",
    "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
    "              colsample_bylevel=1, colsample_bynode=1,\n",
    "              colsample_bytree=0.9870944989347749, early_stopping_rounds=50,\n",
    "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
    "              gamma=0.1383762861356536, gpu_id=-1, grow_policy='depthwise',\n",
    "              importance_type=None, interaction_constraints='',\n",
    "              learning_rate=0.13664139307301595, max_bin=256,\n",
    "              max_cat_threshold=64, max_cat_to_onehot=4, max_delta_step=0,\n",
    "              max_depth=4, max_leaves=0, min_child_weight=4.808561584650579,\n",
    "              missing=nan, monotone_constraints='()', n_estimators=500,\n",
    "              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=42, ...)\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> metrics.roc_auc_score(y_test, xgb_tuned.predict(X_test))\n",
    "0.7629510328319394\n",
    "```\n",
    "\n",
    "### Summary\n",
    "\n",
    "### Exercises\n",
    "\n",
    "## Model Interpretation\n",
    "\n",
    "### Logistic Regression Interpretation\n",
    "\n",
    "``` pycon\n",
    ">>> from sklearn import linear_model, preprocessing\n",
    ">>> std = preprocessing.StandardScaler()\n",
    ">>> lr = linear_model.LogisticRegression(penalty=None)\n",
    ">>> lr.fit(std.fit_transform(X_train), y_train)\n",
    ">>> lr.score(std.transform(X_test), y_test)\n",
    "0.7337016574585635\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> lr.coef_\n",
    "array([[-1.56018160e-01, -4.01817103e-01,  6.01542610e-01,\n",
    "        -1.45213121e-01, -8.13849902e-02, -6.03727624e-01,\n",
    "         3.11683777e-02,  3.16120596e-02, -3.14510213e-02,\n",
    "        -4.59272439e-04, -8.21683100e-03, -5.27737710e-02,\n",
    "        -4.48524110e-03,  1.01853988e-01,  3.49376790e-01,\n",
    "        -1.79149729e-01,  2.41389081e-02, -3.37424750e-01]])\n",
    "```\n",
    "\n",
    "``` python\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "(pd.Series(lr.coef_[0], index=X_train.columns)\n",
    " .sort_values()\n",
    " .plot.barh(ax=ax)\n",
    ")\n",
    "```\n",
    "\n",
    "### Decision Tree Interpretation\n",
    "\n",
    "``` pycon\n",
    ">>> tree7 = tree.DecisionTreeClassifier(max_depth=7)\n",
    ">>> tree7.fit(X_train, y_train)\n",
    ">>> tree7.score(X_test, y_test)\n",
    "0.7337016574585635\n",
    "```\n",
    "\n",
    "``` python\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "(pd.Series(tree7.feature_importances_, index=X_train.columns)\n",
    " .sort_values()\n",
    " .plot.barh(ax=ax)\n",
    ")\n",
    "```\n",
    "\n",
    "``` python\n",
    "import dtreeviz\n",
    "dt3 = tree.DecisionTreeClassifier(max_depth=3)\n",
    "dt3.fit(X_train, y_train)\n",
    "\n",
    "viz = dtreeviz.model(dt3, X_train=X_train, y_train=y_train, \n",
    "    feature_names=list(X_train.columns), target_name='Job',\n",
    "    class_names=['DS', 'SE'])\n",
    "viz.view()\n",
    "```\n",
    "\n",
    "### XGBoost Feature Importance\n",
    "\n",
    "``` python\n",
    "xgb_def = xgb.XGBClassifier()\n",
    "xgb_def.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "``` python\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "(pd.Series(xgb_def.feature_importances_, index=X_train.columns)\n",
    " .sort_values()\n",
    " .plot.barh(ax=ax)\n",
    ")\n",
    "```\n",
    "\n",
    "``` python\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "xgb.plot_importance(xgb_def, importance_type='cover', ax=ax)\n",
    "```\n",
    "\n",
    "### Surrogate Models\n",
    "\n",
    "``` python\n",
    "from sklearn import tree\n",
    "\n",
    "sur_reg_sk = tree.DecisionTreeRegressor(max_depth=4)\n",
    "sur_reg_sk.fit(X_train, xgb_def.predict_proba(X_train)[:,-1])\n",
    "```\n",
    "\n",
    "``` python\n",
    "```\n",
    "\n",
    "### Summary\n",
    "\n",
    "### Exercises\n",
    "\n",
    "## xgbfir (Feature Interactions Reshaped)\n",
    "\n",
    "### Feature Interactions\n",
    "\n",
    "### xgbfir\n",
    "\n",
    "``` python\n",
    "import xgbfir\n",
    "xgbfir.saveXgbFI(xgb_def, feature_names=X_train.columns, OutputXlsxFile='fir.xlsx')\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> fir = pd.read_excel('fir.xlsx')\n",
    ">>> print(fir\n",
    "...  .sort_values(by='Average Rank')\n",
    "...  .head()\n",
    "...  .round(1)\n",
    "... )\n",
    "    Interaction   Gain  FScore  ...  Average Rank  Average Tree Index  \\\n",
    "2             r  517.8      84  ...           3.3                44.6   \n",
    "0     years_exp  597.0     627  ...           4.5                45.1   \n",
    "5     education  296.0     254  ...           4.5                45.2   \n",
    "1  compensation  518.5     702  ...           4.8                47.5   \n",
    "4      major_cs  327.1      96  ...           5.5                48.9   \n",
    "\n",
    "   Average Tree Depth  \n",
    "2                 2.6  \n",
    "0                 3.7  \n",
    "5                 3.3  \n",
    "1                 3.7  \n",
    "4                 3.6  \n",
    "\n",
    "[5 rows x 16 columns]\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> print(pd.read_excel('fir.xlsx', sheet_name='Interaction Depth 1').iloc[:20]\n",
    "...    .sort_values(by='Average Rank')     \n",
    "...    .head(10)          \n",
    "...    .round(1)          \n",
    "... )          \n",
    "               Interaction    Gain  FScore  wFScore  Average wFScore  \\\n",
    "1      education|years_exp   523.8     106     14.8              0.1   \n",
    "0               major_cs|r  1210.8      15      5.4              0.4   \n",
    "6   compensation|education   207.2     103     18.8              0.2   \n",
    "11           age|education   133.2      80     27.2              0.3   \n",
    "3       major_cs|years_exp   441.3      36      4.8              0.1   \n",
    "5            age|years_exp   316.3     216     43.9              0.2   \n",
    "4         age|compensation   344.7     219     38.8              0.2   \n",
    "15    major_stat|years_exp    97.7      32      6.7              0.2   \n",
    "14             education|r   116.5      14      4.6              0.3   \n",
    "18                 age|age    90.5      66     24.7              0.4   \n",
    "\n",
    "    Average Gain  Expected Gain  Gain Rank  FScore Rank  wFScore Rank  \\\n",
    "1            4.9           77.9          2            5             8   \n",
    "0           80.7          607.6          1           45            20   \n",
    "6            2.0           34.0          7            6             7   \n",
    "11           1.7           25.6         12            8             4   \n",
    "3           12.3          108.2          4           20            25   \n",
    "5            1.5           44.0          6            3             1   \n",
    "4            1.6           30.6          5            2             2   \n",
    "15           3.1           20.4         16           25            15   \n",
    "14           8.3           72.3         15           52            27   \n",
    "18           1.4           16.6         19           11             6   \n",
    "\n",
    "    Avg wFScore Rank  Avg Gain Rank  Expected Gain Rank  Average Rank  \\\n",
    "1                 43              8                   3          11.5   \n",
    "0                  8              1                   1          12.7   \n",
    "6                 32             25                   9          14.3   \n",
    "11                12             40                  13          14.8   \n",
    "3                 46              3                   2          16.7   \n",
    "5                 26             57                   7          16.7   \n",
    "4                 34             48                  11          17.0   \n",
    "15                24             14                  14          18.0   \n",
    "14                13              5                   4          19.3   \n",
    "18                 7             62                  16          20.2   \n",
    "\n",
    "    Average Tree Index  Average Tree Depth  \n",
    "1                 38.0                 3.5  \n",
    "0                 12.3                 1.6  \n",
    "6                 50.6                 3.7  \n",
    "11                38.8                 3.6  \n",
    "3                 29.2                 3.2  \n",
    "5                 45.6                 3.9  \n",
    "4                 48.9                 3.9  \n",
    "15                25.5                 3.1  \n",
    "14                40.4                 2.4  \n",
    "18                48.0                 3.6  \n",
    "```\n",
    "\n",
    "``` python\n",
    "(X_train\n",
    " .assign(software_eng=y_train)\n",
    " .corr(method='spearman')\n",
    " .loc[:, ['education', 'years_exp', 'major_cs', 'r', 'compensation', 'age']]\n",
    " .style\n",
    " .background_gradient(cmap='RdBu', vmin=-1, vmax=1)\n",
    " .format('{:.2f}')\n",
    ")\n",
    "```\n",
    "\n",
    "``` python\n",
    "import seaborn as sns\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "sns.heatmap(X_train       \n",
    "            .assign(software_eng=y_train)\n",
    "            .corr(method='spearman')\n",
    "            .loc[:, ['age','education', 'years_exp', 'compensation', 'r', \n",
    "                     'major_cs', 'software_eng']],\n",
    "            cmap='RdBu', annot=True, fmt='.2f', vmin=-1, vmax=1, ax=ax\n",
    ")\n",
    "```\n",
    "\n",
    "``` python\n",
    "import seaborn.objects as so\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "(so\n",
    " .Plot(X_train.assign(software_eng=y_train), x='years_exp', y='education', \n",
    "       color='software_eng')\n",
    " .add(so.Dots(alpha=.9, pointsize=2), so.Jitter(x=.7, y=1))\n",
    " .add(so.Line(), so.PolyFit())\n",
    " .scale(color='viridis')\n",
    " .on(fig)  # not required unless saving to image\n",
    " .plot()   # ditto\n",
    ")\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> print(X_train\n",
    "...  .assign(software_eng=y_train)\n",
    "...  .groupby(['software_eng', 'r', 'major_cs'])\n",
    "...  .age\n",
    "...  .count()\n",
    "...  .unstack()\n",
    "...  .unstack()\n",
    "... )\n",
    "major_cs        0         1     \n",
    "r               0    1    0    1\n",
    "software_eng                    \n",
    "0             410  390  243  110\n",
    "1             308   53  523   73\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> both = (X_train\n",
    "...  .assign(software_eng=y_train)\n",
    "... )\n",
    ">>> print(pd.crosstab(index=both.software_eng, columns=[both.major_cs, both.r]))\n",
    "major_cs        0         1     \n",
    "r               0    1    0    1\n",
    "software_eng                    \n",
    "0             410  390  243  110\n",
    "1             308   53  523   73\n",
    "```\n",
    "\n",
    "``` python\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "grey = '#999999'\n",
    "blue = '#16a2c6'\n",
    "font = 'Roboto'\n",
    "\n",
    "data = (X_train\n",
    " .assign(software_eng=y_train)\n",
    " .groupby(['software_eng', 'r', 'major_cs'])\n",
    " .age\n",
    " .count()\n",
    " .unstack()\n",
    " .unstack())\n",
    "\n",
    "(data\n",
    " .pipe(lambda adf: adf.iloc[:,-2:].plot(color=[grey,blue], linewidth=4, ax=ax, \n",
    "                                        legend=None) and adf)\n",
    " .plot(color=[grey, blue, grey, blue], ax=ax, legend=None)\n",
    ")\n",
    "\n",
    "ax.set_xticks([0, 1], ['Data Scientist', 'Software Engineer'], font=font, size=12, \n",
    "              weight=600)\n",
    "ax.set_yticks([])\n",
    "ax.set_xlabel('')\n",
    "ax.text(x=0, y=.93, s=\"Count Data Scientist or Software Engineer by R/CS\", \n",
    "        transform=fig.transFigure, ha='left', font=font, fontsize=10, weight=1000)\n",
    "ax.text(x=0, y=.83, s=\"(Studied CS) Thick lines\\n(R) Blue\", transform=fig.transFigure, \n",
    "        ha='left', font=font, fontsize=10, weight=300)\n",
    "for side in 'left,top,right,bottom'.split(','):\n",
    "    ax.spines[side].set_visible(False)  \n",
    "# labels\n",
    "for left,txt in zip(data.iloc[0], ['Other/No R', 'Other/R', 'CS/No R', 'CS/R']):\n",
    "    ax.text(x=-.02, y=left, s=f'{txt} ({left})', ha='right', va='center', \n",
    "            font=font, weight=300)\n",
    "for right,txt in zip(data.iloc[1], ['Other/No R', 'Other/R', 'CS/No R', 'CS/R']):\n",
    "    ax.text(x=1.02, y=right, s=f'{txt} ({right})', ha='left', va='center', \n",
    "            font=font, weight=300)\n",
    "```\n",
    "\n",
    "### Deeper Interactions\n",
    "\n",
    "``` pycon\n",
    ">>> print(pd.read_excel('fir.xlsx', sheet_name='Interaction Depth 2').iloc[:20]\n",
    "...   .sort_values(by='Average Rank')          \n",
    "...   .head(5)          \n",
    "... )          \n",
    "                         Interaction         Gain  FScore  ...  Average Rank  \\\n",
    "0               major_cs|r|years_exp  1842.711375      17  ...     12.000000   \n",
    "7            age|education|years_exp   267.537987      53  ...     15.666667   \n",
    "13        age|compensation|education   154.313245      55  ...     15.833333   \n",
    "2   compensation|education|years_exp   431.541357      91  ...     17.166667   \n",
    "14             education|r|years_exp   145.534591      17  ...     19.000000   \n",
    "\n",
    "    Average Tree Index  Average Tree Depth  \n",
    "0             2.588235            2.117647  \n",
    "7            31.452830            3.981132  \n",
    "13           47.381818            3.800000  \n",
    "2            47.175824            4.010989  \n",
    "14           34.352941            2.588235  \n",
    "\n",
    "[5 rows x 16 columns]\n",
    "```\n",
    "\n",
    "### Specifying Feature Interactions\n",
    "\n",
    "``` python\n",
    "constraints = [['education', 'years_exp'], ['major_cs', 'r'],\n",
    "   ['compensation', 'education'], ['age', 'education'],\n",
    "   ['major_cs', 'years_exp'], ['age', 'years_exp'],\n",
    "   ['age', 'compensation'], ['major_stat', 'years_exp'],\n",
    "]\n",
    "```\n",
    "\n",
    "``` python\n",
    "def flatten(seq):\n",
    "    res = []\n",
    "    for sub in seq:\n",
    "        res.extend(sub)\n",
    "    return res\n",
    "\n",
    "\n",
    "small_cols = sorted(set(flatten(constraints)))\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> print(small_cols)\n",
    "['age', 'compensation', 'education', 'major_cs', 'major_stat', 'r', 'years_exp']\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> xg_constraints = xgb.XGBClassifier(interaction_constraints=constraints)\n",
    ">>> xg_constraints.fit(X_train.loc[:, small_cols], y_train)\n",
    ">>> xg_constraints.score(X_test.loc[:, small_cols], y_test)\n",
    "\n",
    "0.7259668508287292\n",
    "```\n",
    "\n",
    "``` python\n",
    "my_dot_export(xg_constraints, num_trees=0, filename='img/constrains0_xg.dot', \n",
    "              title='First Constrained Tree')    \n",
    "```\n",
    "\n",
    "### Summary\n",
    "\n",
    "### Exercises\n",
    "\n",
    "## Exploring SHAP\n",
    "\n",
    "### SHAP\n",
    "\n",
    "``` python\n",
    "step_params = {'random_state': 42,\n",
    " 'max_depth': 5,\n",
    " 'min_child_weight': 0.6411044640540848,\n",
    " 'subsample': 0.9492383155577023,\n",
    " 'colsample_bytree': 0.6235721099295888,\n",
    " 'gamma': 0.00011273797329538491,\n",
    " 'learning_rate': 0.24399020050740935}\n",
    "xg_step = xgb.XGBClassifier(**step_params, early_stopping_rounds=50,\n",
    "                            n_estimators=500)\n",
    "xg_step.fit(X_train, y_train,\n",
    "       eval_set=[(X_train, y_train),\n",
    "                 (X_test, y_test)\n",
    "                ]\n",
    "      )\n",
    "```\n",
    "\n",
    "``` python\n",
    "import shap\n",
    "shap.initjs()\n",
    "\n",
    "shap_ex = shap.TreeExplainer(xg_step)\n",
    "vals = shap_ex(X_test)\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> shap_df = pd.DataFrame(vals.values, columns=X_test.columns)\n",
    ">>> print(shap_df)\n",
    "          age  education  years_exp  compensation    python         r  \\\n",
    "0    0.426614   0.390184  -0.246353      0.145825 -0.034680  0.379261   \n",
    "1    0.011164  -0.131144  -0.292135     -0.014521  0.016003 -1.043464   \n",
    "2   -0.218063  -0.140705  -0.411293      0.048281  0.424516  0.487451   \n",
    "3   -0.015227  -0.299068  -0.426323     -0.205840 -0.125867  0.320594   \n",
    "4   -0.468785  -0.200953  -0.230639      0.064272  0.021362  0.355619   \n",
    "..        ...        ...        ...           ...       ...       ...   \n",
    "900  0.268237  -0.112710   0.330096     -0.209942  0.012074 -1.144335   \n",
    "901  0.154642   0.572190  -0.227121      0.448253 -0.057847  0.290381   \n",
    "902  0.079129  -0.095771   1.136799      0.150705  0.133260  0.484103   \n",
    "903 -0.206584   0.430074  -0.385100     -0.078808 -0.083052 -0.992487   \n",
    "904  0.007351   0.589351   1.485712      0.056398 -0.047231  0.373149   \n",
    "\n",
    "          sql   Q1_Male  Q1_Female  Q1_Prefer not to say  \\\n",
    "0   -0.019017  0.004868   0.000877              0.002111   \n",
    "1    0.020524  0.039019   0.047712              0.001010   \n",
    "2   -0.098703 -0.004710   0.063545              0.000258   \n",
    "3   -0.062712  0.019110   0.012257              0.002184   \n",
    "4   -0.083344 -0.017202   0.002754              0.001432   \n",
    "..        ...       ...        ...                   ...   \n",
    "900 -0.065815  0.028274   0.032291              0.001012   \n",
    "901 -0.069114  0.006243   0.007443              0.002198   \n",
    "902 -0.120819  0.012034   0.057516              0.000266   \n",
    "903 -0.088811  0.080561   0.028648              0.000876   \n",
    "904 -0.105290  0.029283   0.074762              0.001406   \n",
    "\n",
    "     Q1_Prefer to self-describe  Q3_United States of America  Q3_India  \\\n",
    "0                           0.0                     0.033738 -0.117918   \n",
    "1                           0.0                     0.068171  0.086444   \n",
    "2                           0.0                     0.005533 -0.105534   \n",
    "3                           0.0                    -0.000044  0.042814   \n",
    "4                           0.0                     0.035772 -0.073206   \n",
    "..                          ...                          ...       ...   \n",
    "900                         0.0                    -0.086408  0.136677   \n",
    "901                         0.0                    -0.074364  0.115520   \n",
    "902                         0.0                     0.103810 -0.097848   \n",
    "903                         0.0                     0.045213  0.066553   \n",
    "904                         0.0                    -0.031587  0.117050   \n",
    "\n",
    "     Q3_China  major_cs  major_other  major_eng  major_stat  \n",
    "0   -0.018271  0.369876     0.014006  -0.013465    0.104177  \n",
    "1   -0.026271 -0.428484    -0.064157  -0.026041    0.069931  \n",
    "2   -0.010548 -0.333695     0.016919  -0.026932   -0.591922  \n",
    "3   -0.024099  0.486864     0.038438  -0.013727    0.047564  \n",
    "4   -0.022188  0.324419     0.012664  -0.019550    0.093926  \n",
    "..        ...       ...          ...        ...         ...  \n",
    "900  0.310404 -0.407444    -0.013195  -0.026412   -0.484734  \n",
    "901 -0.008244  0.602087     0.039680  -0.012820    0.083934  \n",
    "902  0.003234 -0.313785    -0.080046  -0.066032    0.101975  \n",
    "903 -0.031448 -0.524141    -0.048108  -0.007185    0.093196  \n",
    "904  0.008734 -0.505613    -0.159411  -0.067388    0.126560  \n",
    "\n",
    "[905 rows x 18 columns]\n",
    "          age  education  years_exp  compensation    python         r  \\\n",
    "0    0.426614   0.390184  -0.246353      0.145825 -0.034680  0.379261   \n",
    "1    0.011164  -0.131144  -0.292135     -0.014521  0.016003 -1.043464   \n",
    "2   -0.218063  -0.140705  -0.411293      0.048281  0.424516  0.487451   \n",
    "3   -0.015227  -0.299068  -0.426323     -0.205840 -0.125867  0.320594   \n",
    "4   -0.468785  -0.200953  -0.230639      0.064272  0.021362  0.355619   \n",
    "..        ...        ...        ...           ...       ...       ...   \n",
    "900  0.268237  -0.112710   0.330096     -0.209942  0.012074 -1.144335   \n",
    "901  0.154642   0.572190  -0.227121      0.448253 -0.057847  0.290381   \n",
    "902  0.079129  -0.095771   1.136799      0.150705  0.133260  0.484103   \n",
    "903 -0.206584   0.430074  -0.385100     -0.078808 -0.083052 -0.992487   \n",
    "904  0.007351   0.589351   1.485712      0.056398 -0.047231  0.373149   \n",
    "\n",
    "          sql   Q1_Male  Q1_Female  Q1_Prefer not to say  \\\n",
    "0   -0.019017  0.004868   0.000877              0.002111   \n",
    "1    0.020524  0.039019   0.047712              0.001010   \n",
    "2   -0.098703 -0.004710   0.063545              0.000258   \n",
    "3   -0.062712  0.019110   0.012257              0.002184   \n",
    "4   -0.083344 -0.017202   0.002754              0.001432   \n",
    "..        ...       ...        ...                   ...   \n",
    "900 -0.065815  0.028274   0.032291              0.001012   \n",
    "901 -0.069114  0.006243   0.007443              0.002198   \n",
    "902 -0.120819  0.012034   0.057516              0.000266   \n",
    "903 -0.088811  0.080561   0.028648              0.000876   \n",
    "904 -0.105290  0.029283   0.074762              0.001406   \n",
    "\n",
    "     Q1_Prefer to self-describe  Q3_United States of America  Q3_India  \\\n",
    "0                           0.0                     0.033738 -0.117918   \n",
    "1                           0.0                     0.068171  0.086444   \n",
    "2                           0.0                     0.005533 -0.105534   \n",
    "3                           0.0                    -0.000044  0.042814   \n",
    "4                           0.0                     0.035772 -0.073206   \n",
    "..                          ...                          ...       ...   \n",
    "900                         0.0                    -0.086408  0.136677   \n",
    "901                         0.0                    -0.074364  0.115520   \n",
    "902                         0.0                     0.103810 -0.097848   \n",
    "903                         0.0                     0.045213  0.066553   \n",
    "904                         0.0                    -0.031587  0.117050   \n",
    "\n",
    "     Q3_China  major_cs  major_other  major_eng  major_stat  \n",
    "0   -0.018271  0.369876     0.014006  -0.013465    0.104177  \n",
    "1   -0.026271 -0.428484    -0.064157  -0.026041    0.069931  \n",
    "2   -0.010548 -0.333695     0.016919  -0.026932   -0.591922  \n",
    "3   -0.024099  0.486864     0.038438  -0.013727    0.047564  \n",
    "4   -0.022188  0.324419     0.012664  -0.019550    0.093926  \n",
    "..        ...       ...          ...        ...         ...  \n",
    "900  0.310404 -0.407444    -0.013195  -0.026412   -0.484734  \n",
    "901 -0.008244  0.602087     0.039680  -0.012820    0.083934  \n",
    "902  0.003234 -0.313785    -0.080046  -0.066032    0.101975  \n",
    "903 -0.031448 -0.524141    -0.048108  -0.007185    0.093196  \n",
    "904  0.008734 -0.505613    -0.159411  -0.067388    0.126560  \n",
    "\n",
    "[905 rows x 18 columns]\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> print(pd.concat([shap_df.sum(axis='columns')\n",
    "...                        .rename('pred') + vals.base_values,\n",
    "...    pd.Series(y_test, name='true')], axis='columns')\n",
    "...    .assign(prob=lambda adf: (np.exp(adf.pred) / \n",
    "...                              (1 + np.exp(adf.pred))))\n",
    "... )     \n",
    "         pred  true      prob\n",
    "0    1.204692     1  0.769358\n",
    "1   -2.493559     0  0.076311\n",
    "2   -2.205473     0  0.099260\n",
    "3   -0.843847     1  0.300725\n",
    "4   -0.168726     1  0.457918\n",
    "..        ...   ...       ...\n",
    "900 -1.698727     0  0.154632\n",
    "901  1.957872     0  0.876302\n",
    "902  0.786588     0  0.687098\n",
    "903 -2.299702     0  0.091148\n",
    "904  1.497035     1  0.817132\n",
    "\n",
    "[905 rows x 3 columns]\n",
    "```\n",
    "\n",
    "### Examining a Single Prediction\n",
    "\n",
    "``` pycon\n",
    ">>> X_test.iloc[0]\n",
    "age                            22.0\n",
    "education                      16.0\n",
    "years_exp                       1.0\n",
    "compensation                    0.0\n",
    "python                          1.0\n",
    "r                               0.0\n",
    "sql                             0.0\n",
    "Q1_Male                         1.0\n",
    "Q1_Female                       0.0\n",
    "Q1_Prefer not to say            0.0\n",
    "Q1_Prefer to self-describe      0.0\n",
    "Q3_United States of America     0.0\n",
    "Q3_India                        1.0\n",
    "Q3_China                        0.0\n",
    "major_cs                        1.0\n",
    "major_other                     0.0\n",
    "major_eng                       0.0\n",
    "major_stat                      0.0\n",
    "Name: 7894, dtype: float64\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> # predicts software engineer... why?\n",
    ">>> xg_step.predict(X_test.iloc[[0]])  \n",
    "array([1])\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> # ground truth\n",
    ">>> y_test[0]\n",
    "1\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> # Since this is below zero, the default is Data Scientist\n",
    ">>> shap_ex.expected_value\n",
    "-0.2166416\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> # > 0 therefore ... Software Engineer\n",
    ">>> shap_ex.expected_value + vals.values[0].sum()\n",
    "1.2046916\n",
    "```\n",
    "\n",
    "### Waterfall Plots\n",
    "\n",
    "``` python\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "shap.plots.waterfall(vals[0], show=False)\n",
    "```\n",
    "\n",
    "``` python\n",
    "def plot_histograms(df, columns, row=None, title='', color='shap'):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The DataFrame to plot histograms for.\n",
    "    columns : list of str\n",
    "        The names of the columns to plot histograms for.\n",
    "    row : pandas.Series, optional\n",
    "        A row of data to plot a vertical line for.\n",
    "    title : str, optional\n",
    "        The title to use for the figure.\n",
    "    color : str, optional\n",
    "        'shap' - color positive values red. Negative blue\n",
    "        'mean' - above mean red. Below blue.\n",
    "        None - black\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    matplotlib.figure.Figure\n",
    "        The figure object containing the histogram plots.    \n",
    "    \"\"\"\n",
    "    red = '#ff0051'\n",
    "    blue = '#008bfb'\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    hist = (df\n",
    "     [columns]\n",
    "     .hist(ax=ax, color='#bbb')\n",
    "    )\n",
    "    fig = hist[0][0].get_figure()\n",
    "    if row is not None:\n",
    "        name2ax = {ax.get_title():ax for ax in fig.axes}\n",
    "        pos, neg = red, blue\n",
    "        if color is None:\n",
    "            pos, neg = 'black', 'black'\n",
    "        for column in columns:\n",
    "            if color == 'mean':\n",
    "                mid = df[column].mean()\n",
    "            else:\n",
    "                mid = 0\n",
    "            if row[column] > mid:\n",
    "                c = pos\n",
    "            else:\n",
    "                c = neg\n",
    "            name2ax[column].axvline(row[column], c=c)\n",
    "    fig.tight_layout()\n",
    "    fig.suptitle(title)\n",
    "    return fig    \n",
    "```\n",
    "\n",
    "``` python\n",
    "features = ['education', 'r', 'major_cs', 'age', 'years_exp', \n",
    "           'compensation']\n",
    "fig = plot_histograms(shap_df, features, shap_df.iloc[0], \n",
    "                      title='SHAP values for row 0')\n",
    "```\n",
    "\n",
    "``` python\n",
    "fig = plot_histograms(X_test, features, X_test.iloc[0], \n",
    "                      title='Values for row 0', color='mean')\n",
    "```\n",
    "\n",
    "``` python\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "(pd.Series(vals.values[0], index=X_test.columns)\n",
    " .sort_values(key=np.abs)\n",
    " .plot.barh(ax=ax)\n",
    ")\n",
    "```\n",
    "\n",
    "### A Force Plot\n",
    "\n",
    "``` python\n",
    "# use matplotlib if having js issues\n",
    "# blue - DS\n",
    "# red - Software Engineer\n",
    "# to save need both matplotlib=True, show=False\n",
    "res = shap.plots.force(base_value=vals.base_values, \n",
    "                      shap_values=vals.values[0,:], features=X_test.iloc[0], \n",
    "                      matplotlib=True, show=False\n",
    ")\n",
    "res.savefig('img/shap_forceplot0.png', dpi=600, bbox_inches='tight')\n",
    "```\n",
    "\n",
    "### Force Plot with Multiple Predictions\n",
    "\n",
    "``` python\n",
    "# First n values\n",
    "n = 100\n",
    "# blue - DS\n",
    "# red - Software Engineer\n",
    "shap.plots.force(base_value=vals.base_values, \n",
    "               shap_values=vals.values[:n,:], features=X_test.iloc[:n], \n",
    "               )\n",
    "```\n",
    "\n",
    "### Understanding Features with Dependence Plots\n",
    "\n",
    "``` python\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "shap.plots.scatter(vals[:, 'education'], ax=ax, color=vals, \n",
    "                   x_jitter=0, hist=False)\n",
    "```\n",
    "\n",
    "### Jittering a Dependence Plot\n",
    "\n",
    "``` python\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "shap.plots.scatter(vals[:, 'education'], ax=ax, color=vals[:, 'years_exp'], x_jitter=1,\n",
    "                   alpha=.5)\n",
    "```\n",
    "\n",
    "``` python\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "shap.plots.scatter(vals[:, 'major_cs'], ax=ax, color=vals[:, 'r'], alpha=.5)\n",
    "```\n",
    "\n",
    "### Heatmaps and Correlations\n",
    "\n",
    "``` python\n",
    "import seaborn as sns\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "sns.heatmap(X_test       \n",
    "            .assign(software_eng=y_test)\n",
    "            .corr(method='spearman')\n",
    "            .loc[:, ['age', 'education', 'years_exp',\n",
    "                     'compensation', 'r', 'major_cs', \n",
    "                     'software_eng']],\n",
    "            cmap='RdBu', annot=True, fmt='.2f', vmin=-1, vmax=1, ax=ax\n",
    ")\n",
    "```\n",
    "\n",
    "``` python\n",
    "import seaborn as sns\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "sns.heatmap(shap_df       \n",
    "            .assign(software_eng=y_test)\n",
    "            .corr(method='spearman')\n",
    "            .loc[:, ['age', 'education', 'years_exp',  'compensation', 'r', 'major_cs',\n",
    "                     'software_eng']],\n",
    "            cmap='RdBu', annot=True, fmt='.2f', vmin=-1, vmax=1, ax=ax\n",
    ")\n",
    "```\n",
    "\n",
    "### Beeswarm Plots of Global Behavior\n",
    "\n",
    "``` python\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "shap.plots.beeswarm(vals)\n",
    "```\n",
    "\n",
    "``` python\n",
    "from matplotlib import cm\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "shap.plots.beeswarm(vals, max_display=len(X_test.columns), color=cm.autumn_r)\n",
    "```\n",
    "\n",
    "### SHAP with No Interaction\n",
    "\n",
    "``` python\n",
    "no_int_params = {'random_state': 42,\n",
    "                 'max_depth': 1\n",
    "}\n",
    "xg_no_int = xgb.XGBClassifier(**no_int_params, early_stopping_rounds=50,\n",
    "                              n_estimators=500)\n",
    "xg_no_int.fit(X_train, y_train,\n",
    "       eval_set=[(X_train, y_train),\n",
    "                 (X_test, y_test)\n",
    "                ]\n",
    ")\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> xg_no_int.score(X_test, y_test)\n",
    "0.7370165745856354\n",
    "```\n",
    "\n",
    "``` python\n",
    "shap_ind = shap.TreeExplainer(xg_no_int)\n",
    "shap_ind_vals = shap_ind(X_test)\n",
    "```\n",
    "\n",
    "``` python\n",
    "from matplotlib import cm\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "shap.plots.beeswarm(shap_ind_vals, max_display=len(X_test.columns))\n",
    "```\n",
    "\n",
    "``` python\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "shap.plots.scatter(vals[:, 'years_exp'], ax=ax, \n",
    "                   color=vals[:, 'age'], alpha=.5,\n",
    "                   x_jitter=1)\n",
    "```\n",
    "\n",
    "``` python\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "shap.plots.scatter(shap_ind_vals[:, 'years_exp'], ax=ax,\n",
    "                   color=shap_ind_vals[:, 'age'], alpha=.5,\n",
    "                   x_jitter=1)\n",
    "```\n",
    "\n",
    "### Summary\n",
    "\n",
    "### Exercises\n",
    "\n",
    "## Better Models with ICE, Partial Dependence, Monotonic Constraints, and Calibration\n",
    "\n",
    "### ICE Plots\n",
    "\n",
    "``` python\n",
    "xgb_def = xgb.XGBClassifier(random_state=42)\n",
    "xgb_def.fit(X_train, y_train)\n",
    "xgb_def.score(X_test, y_test)\n",
    "```\n",
    "\n",
    "``` python\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(8,4))\n",
    "PartialDependenceDisplay.from_estimator(xgb_def, X_train, features=['r', 'education'],\n",
    "                                       kind='individual', ax=axes)\n",
    "```\n",
    "\n",
    "``` python\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(8,4))\n",
    "PartialDependenceDisplay.from_estimator(xgb_def, X_train, features=['r', 'education'],\n",
    "                                       centered=True,\n",
    "                                       kind='individual', ax=axes)\n",
    "```\n",
    "\n",
    "``` python\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(8,4))\n",
    "ax_h0 = axes[0].twinx()\n",
    "ax_h0.hist(X_train.r, zorder=0)\n",
    "\n",
    "ax_h1 = axes[1].twinx()\n",
    "ax_h1.hist(X_train.education, zorder=0)\n",
    "\n",
    "PartialDependenceDisplay.from_estimator(xgb_def, X_train, features=['r', 'education'],\n",
    "                                        centered=True,\n",
    "                                        ice_lines_kw={'zorder':10},\n",
    "                                        kind='individual', ax=axes)\n",
    "fig.tight_layout()\n",
    "```\n",
    "\n",
    "``` python\n",
    "def quantile_ice(clf, X, col, center=True, q=10, color='k', alpha=.5, legend=True,\n",
    "                add_hist=False, title='', val_limit=10, ax=None):\n",
    "  \"\"\"\n",
    "    Generate an ICE plot for a binary classifier's predicted probabilities split \n",
    "    by quantiles.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    clf : binary classifier\n",
    "        A binary classifier with a `predict_proba` method.\n",
    "    X : DataFrame\n",
    "        Feature matrix to predict on with shape (n_samples, n_features).\n",
    "    col : str\n",
    "        Name of column in `X` to plot against the quantiles of predicted probabilities.\n",
    "    center : bool, default=True\n",
    "        Whether to center the plot on 0.5.\n",
    "    q : int, default=10\n",
    "        Number of quantiles to split the predicted probabilities into.\n",
    "    color : str or array-like, default='k'\n",
    "        Color(s) of the lines in the plot.\n",
    "    alpha : float, default=0.5\n",
    "        Opacity of the lines in the plot.\n",
    "    legend : bool, default=True\n",
    "        Whether to show the plot legend.\n",
    "    add_hist : bool, default=False\n",
    "        Whether to add a histogram of the `col` variable to the plot.\n",
    "    title : str, default=''\n",
    "        Title of the plot.\n",
    "    val_limit : num, default=10\n",
    "        Maximum number of values to test for col.\n",
    "    ax : Matplotlib Axis, deafault=None\n",
    "        Axis to plot on.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    results : DataFrame\n",
    "        A DataFrame with the same columns as `X`, as well as a `prob` column with \n",
    "        the predicted probabilities of `clf` for each row in `X`, and a `group` \n",
    "        column indicating which quantile group the row belongs to.\n",
    "  \"\"\"                  \n",
    "  probs = clf.predict_proba(X)\n",
    "  df = (X\n",
    "        .assign(probs=probs[:,-1],\n",
    "               p_bin=lambda df_:pd.qcut(df_.probs, q=q, \n",
    "                                        labels=[f'q{n}' for n in range(1,q+1)])\n",
    "               )\n",
    "       )\n",
    "  groups = df.groupby('p_bin')\n",
    "\n",
    "  vals = X.loc[:,col].unique()\n",
    "  if len(vals) > val_limit:\n",
    "    vals = np.linspace(min(vals), max(vals), num=val_limit)\n",
    "  res = []\n",
    "  for name,g in groups:\n",
    "    for val in vals:\n",
    "      this_X = g.loc[:,X.columns].assign(**{col:val})\n",
    "      q_prob = clf.predict_proba(this_X)[:,-1]\n",
    "      res.append(this_X.assign(prob=q_prob, group=name))\n",
    "  results = pd.concat(res, axis='index')     \n",
    "  if ax is None:\n",
    "    fig, ax = plt.subplots(figsize=(8,4))\n",
    "  if add_hist:\n",
    "    back_ax = ax.twinx()\n",
    "    back_ax.hist(X[col], density=True, alpha=.2) \n",
    "  for name, g in results.groupby('group'):\n",
    "    g.groupby(col).prob.mean().plot(ax=ax, label=name, color=color, alpha=alpha)\n",
    "  if legend:\n",
    "    ax.legend()\n",
    "  if title:\n",
    "    ax.set_title(title)\n",
    "  return results\n",
    "```\n",
    "\n",
    "``` python\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "quantile_ice(xgb_def, X_train, 'education', q=10, legend=False, add_hist=True, ax=ax,\n",
    "            title='ICE plot for Age')\n",
    "```\n",
    "\n",
    "### ICE Plots with SHAP\n",
    "\n",
    "``` python\n",
    "import shap\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "  \n",
    "shap.plots.partial_dependence_plot(ind='education', \n",
    "    model=lambda rows: xgb_def.predict_proba(rows)[:,-1],\n",
    "    data=X_train.iloc[0:1000], ice=True, \n",
    "    npoints=(X_train.education.nunique()),\n",
    "    pd_linewidth=0, show=False, ax=ax)\n",
    "ax.set_title('ICE plot (from SHAP)')\n",
    "```\n",
    "\n",
    "### Partial Dependence Plots\n",
    "\n",
    "``` python\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(8,4))\n",
    "\n",
    "PartialDependenceDisplay.from_estimator(xgb_def, X_train, features=['r', 'education'],\n",
    "                                        kind='average', ax=axes)\n",
    "fig.tight_layout()\n",
    "```\n",
    "\n",
    "``` python\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(8,4))\n",
    "\n",
    "PartialDependenceDisplay.from_estimator(xgb_def, X_train, features=['r', 'education'],\n",
    "                                        centered=True, kind='both',\n",
    "                                        ax=axes)\n",
    "fig.tight_layout()\n",
    "```\n",
    "\n",
    "``` python\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(8,4))\n",
    "\n",
    "PartialDependenceDisplay.from_estimator(xgb_def, X_train, features=['years_exp', 'Q1_Male'],\n",
    "                                        centered=True, kind='both',\n",
    "                                        ax=axes)\n",
    "fig.tight_layout()\n",
    "```\n",
    "\n",
    "### PDP with SHAP\n",
    "\n",
    "``` python\n",
    "import shap\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "\n",
    "col = 'years_exp'  \n",
    "shap.plots.partial_dependence_plot(ind=col,\n",
    "                             model=lambda rows: xgb_def.predict_proba(rows)[:,-1],\n",
    "                             data=X_train.iloc[0:1000], ice=False, \n",
    "                             npoints=(X_train[col].nunique()),\n",
    "                             pd_linewidth=2, show=False, ax=ax)\n",
    "ax.set_title('PDP plot (from SHAP)')\n",
    "```\n",
    "\n",
    "``` python\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "\n",
    "col = 'years_exp'  \n",
    "shap.plots.partial_dependence_plot(ind=col, \n",
    "                             model=lambda rows: xgb_def.predict_proba(rows)[:,-1],\n",
    "                             data=X_train.iloc[0:1000], ice=True, \n",
    "                             npoints=(X_train[col].nunique()),\n",
    "                             model_expected_value=True,\n",
    "                             feature_expected_value=True,\n",
    "                             pd_linewidth=2, show=False, ax=ax)\n",
    "ax.set_title('PDP plot (from SHAP) with ICE Plots')\n",
    "```\n",
    "\n",
    "### Monotonic Constraints\n",
    "\n",
    "``` python\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "\n",
    "(X_test\n",
    " .assign(target=y_test)\n",
    " .corr(method='spearman')\n",
    " .iloc[:-1]\n",
    " .loc[:,'target']\n",
    " .sort_values(key=np.abs)\n",
    " .plot.barh(title='Spearman Correlation with Target', ax=ax)\n",
    ")\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> print(X_train\n",
    "... .assign(target=y_train)\n",
    "... .groupby('education')\n",
    "... .mean()\n",
    "... .loc[:, ['age', 'years_exp', 'target']]\n",
    "... )\n",
    "\n",
    "                 age  years_exp    target\n",
    "education                                \n",
    "12.0       30.428571   2.857143  0.714286\n",
    "13.0       30.369565   6.760870  0.652174\n",
    "16.0       25.720867   2.849593  0.605691\n",
    "18.0       28.913628   3.225528  0.393474\n",
    "19.0       27.642857   4.166667  0.571429\n",
    "20.0       35.310638   4.834043  0.174468\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> X_train.education.value_counts()\n",
    "18.0    1042\n",
    "16.0     738\n",
    "20.0     235\n",
    "13.0      46\n",
    "19.0      42\n",
    "12.0       7\n",
    "Name: education, dtype: int64\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> print(raw\n",
    "... .query('Q3.isin([\"United States of America\", \"China\", \"India\"]) '\n",
    "...        'and Q6.isin([\"Data Scientist\", \"Software Engineer\"])') \n",
    "... .query('Q4 == \"Professional degree\"')\n",
    "... .pipe(lambda df_:pd.crosstab(index=df_.Q5, columns=df_.Q6))\n",
    "... )\n",
    " \n",
    "Q6                                                  Data Scientist  \\\n",
    "Q5                                                                   \n",
    "A business discipline (accounting, economics, f...               0   \n",
    "Computer science (software engineering, etc.)                   12   \n",
    "Engineering (non-computer focused)                               6   \n",
    "Humanities (history, literature, philosophy, etc.)               2   \n",
    "I never declared a major                                         0   \n",
    "Mathematics or statistics                                        2   \n",
    "Other                                                            2   \n",
    "Physics or astronomy                                             2   \n",
    "\n",
    "Q6                                                  Software Engineer  \n",
    "Q5                                                                     \n",
    "A business discipline (accounting, economics, f...                  1  \n",
    "Computer science (software engineering, etc.)                      19  \n",
    "Engineering (non-computer focused)                                 10  \n",
    "Humanities (history, literature, philosophy, etc.)                  0  \n",
    "I never declared a major                                            1  \n",
    "Mathematics or statistics                                           1  \n",
    "Other                                                               1  \n",
    "Physics or astronomy                                                1  \n",
    "```\n",
    "\n",
    "``` python\n",
    "xgb_const = xgb.XGBClassifier(random_state=42,\n",
    "          monotone_constraints={'years_exp':1, 'education':-1})\n",
    "xgb_const.fit(X_train, y_train)\n",
    "xgb_const.score(X_test, y_test)\n",
    "```\n",
    "\n",
    "``` python\n",
    "small_cols = ['age', 'education', 'years_exp', 'compensation', 'python', 'r', 'sql',\n",
    "              #'Q1_Male', 'Q1_Female', 'Q1_Prefer not to say',\n",
    "              #'Q1_Prefer to self-describe', \n",
    "              'Q3_United States of America', 'Q3_India',\n",
    "              'Q3_China', 'major_cs', 'major_other', 'major_eng', 'major_stat']\n",
    "xgb_const2 = xgb.XGBClassifier(random_state=42,\n",
    "          monotone_constraints={'years_exp':1, 'education':-1})\n",
    "xgb_const2.fit(X_train[small_cols], y_train)\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> xgb_const2.score(X_test[small_cols], y_test)\n",
    "0.7569060773480663\n",
    "```\n",
    "\n",
    "``` python\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "(pd.Series(xgb_def.feature_importances_, index=X_train.columns)\n",
    " .sort_values()\n",
    " .plot.barh(ax=ax)\n",
    ")\n",
    "```\n",
    "\n",
    "``` python\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "(pd.Series(xgb_const2.feature_importances_, index=small_cols)\n",
    " .sort_values()\n",
    " .plot.barh(ax=ax)\n",
    ")\n",
    "```\n",
    "\n",
    "### Calibrating a Model\n",
    "\n",
    "``` python\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "xgb_cal = CalibratedClassifierCV(xgb_def, method='sigmoid', cv='prefit')\n",
    "xgb_cal.fit(X_test, y_test)\n",
    "\n",
    "xgb_cal_iso = CalibratedClassifierCV(xgb_def, method='isotonic', cv='prefit')\n",
    "xgb_cal_iso.fit(X_test, y_test)\n",
    "```\n",
    "\n",
    "### Calibration Curves\n",
    "\n",
    "``` python\n",
    "from sklearn.calibration import CalibrationDisplay\n",
    "from matplotlib.gridspec import GridSpec\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "gs = GridSpec(4, 3)\n",
    "axes = fig.add_subplot(gs[:2, :3])\n",
    "display = CalibrationDisplay.from_estimator(xgb_def, X_test, y_test, \n",
    "                                            n_bins=10, ax=axes)\n",
    "disp_cal = CalibrationDisplay.from_estimator(xgb_cal, X_test, y_test, \n",
    "                                      n_bins=10,ax=axes, name='sigmoid')\n",
    "disp_cal_iso = CalibrationDisplay.from_estimator(xgb_cal_iso, X_test, y_test, \n",
    "                                      n_bins=10, ax=axes, name='isotonic')\n",
    "row = 2\n",
    "col = 0\n",
    "ax = fig.add_subplot(gs[row, col])\n",
    "ax.hist(display.y_prob, range=(0,1), bins=20)\n",
    "ax.set(title='Default', xlabel='Predicted Prob')\n",
    "ax2 = fig.add_subplot(gs[row, 1])\n",
    "ax2.hist(disp_cal.y_prob, range=(0,1), bins=20)\n",
    "ax2.set(title='Sigmoid', xlabel='Predicted Prob')\n",
    "ax3 = fig.add_subplot(gs[row, 2])\n",
    "ax3.hist(disp_cal_iso.y_prob, range=(0,1), bins=20)\n",
    "ax3.set(title='Isotonic', xlabel='Predicted Prob')\n",
    "fig.tight_layout()\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> xgb_cal.score(X_test, y_test)\n",
    "0.7480662983425415\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> xgb_cal_iso.score(X_test, y_test)\n",
    "0.7491712707182321\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> xgb_def.score(X_test, y_test)\n",
    "0.7458563535911602\n",
    "```\n",
    "\n",
    "### Summary\n",
    "\n",
    "### Exercises\n",
    "\n",
    "## Serving Models with MLFlow\n",
    "\n",
    "### Installation and Setup\n",
    "\n",
    "``` python\n",
    "%matplotlib inline\n",
    "\n",
    "from feature_engine import encoding, imputation\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import base, metrics, model_selection, \\\n",
    "   pipeline, preprocessing\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score  \n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "import urllib\n",
    "import zipfile\n",
    "```\n",
    "\n",
    "``` python\n",
    "import pandas as pd\n",
    "from sklearn import model_selection, preprocessing\n",
    "import xg_helpers as xhelp\n",
    "\n",
    "\n",
    "url = 'https://github.com/mattharrison/datasets/raw/master/data/'\\\n",
    "    'kaggle-survey-2018.zip'\n",
    "fname = 'kaggle-survey-2018.zip'\n",
    "member_name = 'multipleChoiceResponses.csv'\n",
    "\n",
    "raw = xhelp.extract_zip(url, fname, member_name)\n",
    "## Create raw X and raw y\n",
    "kag_X, kag_y = xhelp.get_rawX_y(raw, 'Q6')\n",
    "    \n",
    "## Split data    \n",
    "kag_X_train, kag_X_test, kag_y_train, kag_y_test = \\\n",
    "    model_selection.train_test_split(\n",
    "        kag_X, kag_y, test_size=.3, random_state=42, stratify=kag_y)    \n",
    "\n",
    "## Transform X with pipeline\n",
    "X_train = xhelp.kag_pl.fit_transform(kag_X_train)\n",
    "X_test = xhelp.kag_pl.transform(kag_X_test)\n",
    "\n",
    "## Transform y with label encoder\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "label_encoder.fit(kag_y_train)\n",
    "y_train = label_encoder.transform(kag_y_train)\n",
    "y_test = label_encoder.transform(kag_y_test)\n",
    "\n",
    "# Combined Data for cross validation/etc\n",
    "X = pd.concat([X_train, X_test], axis='index')\n",
    "y = pd.Series([*y_train, *y_test], index=X.index)\n",
    "```\n",
    "\n",
    "``` python\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "import mlflow\n",
    "from sklearn import metrics\n",
    "import xgboost as xgb\n",
    "\n",
    "ex_id = mlflow.create_experiment(name='ex3', artifact_location='ex2path')\n",
    "mlflow.set_experiment(experiment_name='ex3')\n",
    "with mlflow.start_run():\n",
    "    params = {'random_state': 42}\n",
    "    rounds = [{'max_depth': hp.quniform('max_depth', 1, 12, 1),  # tree\n",
    "               'min_child_weight': hp.loguniform('min_child_weight', -2, 3)},\n",
    "              {'subsample': hp.uniform('subsample', 0.5, 1),   # stochastic\n",
    "               'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1)},\n",
    "              {'gamma': hp.loguniform('gamma', -10, 10)}, # regularization\n",
    "              {'learning_rate': hp.loguniform('learning_rate', -7, 0)} # boosting\n",
    "    ]\n",
    "\n",
    "    for round in rounds:\n",
    "        params = {**params, **round}\n",
    "        trials = Trials()\n",
    "        best = fmin(fn=lambda space: xhelp.hyperparameter_tuning(\n",
    "                space, X_train, y_train, X_test, y_test),            \n",
    "            space=params,           \n",
    "            algo=tpe.suggest,            \n",
    "            max_evals=10,            \n",
    "            trials=trials,\n",
    "            timeout=60*5 # 5 minutes\n",
    "        )\n",
    "        params = {**params, **best}\n",
    "        for param, val in params.items():\n",
    "            mlflow.log_param(param, val)\n",
    "        params['max_depth'] = int(params['max_depth'])\n",
    "        xg = xgb.XGBClassifier(eval_metric='logloss', early_stopping_rounds=50, **params)\n",
    "        xg.fit(X_train, y_train,\n",
    "               eval_set=[(X_train, y_train),\n",
    "                         (X_test, y_test)\n",
    "                        ]\n",
    "              )     \n",
    "        for metric in [metrics.accuracy_score, metrics.precision_score, metrics.recall_score, \n",
    "                       metrics.f1_score]:\n",
    "            mlflow.log_metric(metric.__name__, metric(y_test, xg.predict(X_test)))\n",
    "\n",
    "    model_info = mlflow.xgboost.log_model(xg, artifact_path='model')\n",
    "    \n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> ex_id\n",
    "'172212630951564101'\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> model_info.run_id\n",
    "'263b3e793f584251a4e4cd1a2d494110'\n",
    "```\n",
    "\n",
    "### Inspecting Model Artifacts\n",
    "\n",
    "### Running A Model From Code\n",
    "\n",
    "``` python\n",
    "import mlflow\n",
    "logged_model = 'runs:/ecc05fedb5c942598741816a1c6d76e2/model'\n",
    "\n",
    "# Load model as a PyFuncModel.\n",
    "loaded_model = mlflow.pyfunc.load_model(logged_model)\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> loaded_model.predict(X_test.iloc[[0]])\n",
    "array([1])\n",
    "```\n",
    "\n",
    "### Serving Predictions\n",
    "\n",
    "### Querying from the Command Line\n",
    "\n",
    "``` pycon\n",
    ">>> X_test.head(2).to_json(orient='split', index=False)\n",
    "'{\"columns\":[\"age\",\"education\",\"years_exp\",\"compensation\",\n",
    "\"python\",\"r\",\"sql\",\"Q1_Male\",\"Q1_Female\",\"Q1_Prefer not to say\",\n",
    "\"Q1_Prefer to self-describe\",\"Q3_United States of America\",\n",
    "\"Q3_India\",\"Q3_China\",\"major_cs\",\"major_other\",\"major_eng\",\n",
    "\"major_stat\"],\"data\":[[22,16.0,1.0,0,1,0,0,1,0,0,0,0,1,0,1,0,\n",
    "0,0],[25,18.0,1.0,70000,1,1,0,1,0,0,0,1,0,0,0,1,0,0]]}'\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> import json\n",
    ">>> json.loads(X_test.head(2).to_json(orient='split', index=False))\n",
    "{'columns': ['age',\n",
    "  'education',\n",
    "  'years_exp',\n",
    "  'compensation',\n",
    "  'python',\n",
    "  'r',\n",
    "  'sql',\n",
    "  'Q1_Male',\n",
    "  'Q1_Female',\n",
    "  'Q1_Prefer not to say',\n",
    "  'Q1_Prefer to self-describe',\n",
    "  'Q3_United States of America',\n",
    "  'Q3_India',\n",
    "  'Q3_China',\n",
    "  'major_cs',\n",
    "  'major_other',\n",
    "  'major_eng',\n",
    "  'major_stat'],\n",
    " 'data': [[22, 16.0, 1.0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0],\n",
    "  [25, 18.0, 1.0, 70000, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]]}\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> {'dataframe_split': json.loads(X_test.head(2).to_json(orient='split', \n",
    "...                                                       index=False))}\n",
    "{'dataframe_split': {'columns': ['age',\n",
    "   'education',\n",
    "   'years_exp',\n",
    "   'compensation',\n",
    "   'python',\n",
    "   'r',\n",
    "   'sql',\n",
    "   'Q1_Male',\n",
    "   'Q1_Female',\n",
    "   'Q1_Prefer not to say',\n",
    "   'Q1_Prefer to self-describe',\n",
    "   'Q3_United States of America',\n",
    "   'Q3_India',\n",
    "   'Q3_China',\n",
    "   'major_cs',\n",
    "   'major_other',\n",
    "   'major_eng',\n",
    "   'major_stat'],\n",
    "  'data': [[22, 16.0, 1.0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0],\n",
    "   [25, 18.0, 1.0, 70000, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]]}}\n",
    "```\n",
    "\n",
    "``` python\n",
    "def create_post_data(df):\n",
    "    dictionary = json.loads(df\n",
    "       .to_json(orient='split', index=False))\n",
    "    return json.dumps({'dataframe_split': dictionary})\n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> post_data = create_post_data(X_test.head(2))\n",
    ">>> print(post_data)\n",
    "{\"dataframe_split\": {\"columns\": [\"age\", \"education\", \"years_exp\", \"compensation\", \n",
    "   \"python\", \"r\", \"sql\", \"Q1_Male\", \"Q1_Female\", \"Q1_Prefer not to say\", \n",
    "   \"Q1_Prefer to self-describe\", \"Q3_United States of America\", \"Q3_India\", \"Q3_China\", \n",
    "   \"major_cs\", \"major_other\", \"major_eng\", \"major_stat\"], \n",
    " \"data\": [[22, 16.0, 1.0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0], \n",
    "          [25, 18.0, 1.0, 70000, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]]}}\n",
    "```\n",
    "\n",
    "``` python\n",
    "!curl http://127.0.0.1:1234/invocations -X POST -H \\\n",
    "    \"Content-Type:application/json\" --data $post_data \n",
    "```\n",
    "\n",
    "``` pycon\n",
    ">>> quoted = f\"'{post_data}'\"\n",
    ">>> quoted\n",
    "'\\'{\"dataframe_split\": {\"columns\": [\"age\", \"education\", \n",
    "\"years_exp\", \"compensation\", \"python\", \"r\", \"sql\", \"Q1_Male\", \n",
    "\"Q1_Female\", \"Q1_Prefer not to say\", \"Q1_Prefer to self-describe\",\n",
    "\"Q3_United States of America\", \"Q3_India\", \"Q3_China\", \"major_cs\",\n",
    "\"major_other\", \"major_eng\", \"major_stat\"], \"data\": [[22, 16.0, 1.0,\n",
    "0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0], [25, 18.0, 1.0, \n",
    "70000, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]]}}\\''\n",
    "```\n",
    "\n",
    "``` python\n",
    "def create_post_data(df, quote=True):\n",
    "    dictionary = {'dataframe_split': json.loads(df\n",
    "       .to_json(orient='split', index=False))}\n",
    "    if quote:\n",
    "        return f\"'{dictionary}'\"\n",
    "    else:\n",
    "        return dictionary\n",
    "\n",
    "quoted = create_post_data(X_test.head(2))\n",
    "```\n",
    "\n",
    "``` python\n",
    "!curl http://127.0.0.1:1234/invocations -x post -h \\\n",
    "        \"content-type:application/json\" --data $quoted \n",
    "```\n",
    "\n",
    "### Querying with the Requests Library\n",
    "\n",
    "``` pycon\n",
    ">>> import requests as req\n",
    ">>> import json\n",
    "\n",
    ">>> r = req.post('http://127.0.0.1:1234/invocations', \n",
    "...     json=create_post_data(x_test.head(2), quote=False))\n",
    ">>> print(r.text)\n",
    "{\"predictions\": [1, 0]}\n",
    "```\n",
    "\n",
    "### Building with Docker\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "### Exercises"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
